{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "be4b6c1c",
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2915ce0f",
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "from static_grader import grader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f865a1",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "# Spark Miniproject\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc25b183",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Stack Overflow is a collaboratively edited question-and-answer site originally focused on programming topics. Because of the variety of features tracked, including a variety of feedback metrics, it allows for some open-ended analysis of user behavior on the site.\n",
    "\n",
    "Stack Exchange (the parent organization) provides an anonymized [data dump](https://archive.org/details/stackexchange), and we'll use Spark to perform data manipulation, analysis, and machine learning on this data set. As a side note, there's also an online data explorer which allows you to query the data interactively.\n",
    "\n",
    "*Consider*: Do we need to use Spark to work with this data set? What are our alternatives?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c18065b",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Workflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1284fe",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "**All questions in this miniproject can be done locally in this notebook (i.e. on your Jupyter pod).**  \n",
    "\n",
    "You are free to try running on a cloud service, but note that we have no resources to pay for you to try out these services.  (New users often get a limited amount of free credit to try a service.)  Also, the grader library will not be available, so you would have to get your answers into this notebook to submit to the grader.   See the appropriate lecture notebooks for information on how to use cloud services if you want to try them out.\n",
    "\n",
    "Python example workflow when **not** running in a Jupyter notebook:\n",
    "\n",
    "1. Edit source code in your `main.py` file, classes in a separate `classes.py` (class definitions need to be written in a separate file and then included at runtime).\n",
    "1. If you are using a cloud service, in order to make your code more flexible, it's recommended to incorporate command-line arguments that specify the location of the input data and where output should be written.\n",
    "``` python\n",
    "# Command line arguments using sysv or argparse in Python\n",
    "if __name__ == '__main__':\n",
    "    main(ARGS.input_dir, ARGS.output_dir)\n",
    "```\n",
    "1. Run locally using the `spark-submit` program on a chunk using, eg., `$SPARK_HOME/bin/spark-submit --py-files src/classes.py src/main.py data/stats results/stats/`  Note that long jobs using `spark-submit` may not finish before your server gets automatically shut down (our server only checks for running Jupyter notebooks to avoid shutting down).  \n",
    "1. Run on Amazon Web Services (AWS) once your testing and development are done.  Note that you will also have to load all of the input data on an AWS bucket.  (Similar statements apply if you were to use Google Cloud Platform (GCP) or other services.)  \n",
    "\n",
    "General tips when using `spark-submit` or working on a cloud service:\n",
    "* Try `cat output_dir/* | sort -n -t , -k 1.2 -o sorted_output` to concatenate your output files, which will be in `part-xxxxx` format.\n",
    "* You can alternatively access an interactive PySpark shell on your Jupyter pod with this command: `$SPARK_HOME/bin/pyspark`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccf3e24",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Accessing the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc49c77f",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "The data is available on S3 (`s3://dataincubator-course/spark-stack-data`). There are three sub-folders, `allUsers`, `allPosts`, and `allVotes` which contain Gzipped XML.  The `allPosts` sub-folder will contain data with the following format:\n",
    "\n",
    "``` html\n",
    "<row Body=\"&lt;p&gt;I always validate my web pages, and I recommend you do the same BUT many large company websites DO NOT and cannot validate because the importance of the website looking exactly the same on all systems requires rules to be broken. &lt;/p&gt;&#10;&#10;&lt;p&gt;In general, valid websites help your page look good even on odd configurations (like cell phones) so you should always at least try to make it validate.&lt;/p&gt;&#10;\" CommentCount=\"0\" CreationDate=\"2008-10-12T20:26:29.397\" Id=\"195995\" LastActivityDate=\"2008-10-12T20:26:29.397\" OwnerDisplayName=\"Eric Wendelin\" OwnerUserId=\"25066\" ParentId=\"195973\" PostTypeId=\"2\" Score=\"0\" />\n",
    "```\n",
    "\n",
    "Data from the much smaller `stats.stackexchange.com` website (called \"Cross Validated\") is available in the same format on S3 (`s3://dataincubator-course/spark-stats-data`). This smaller data set will be used below in most questions to avoid working with the full data set for every question.\n",
    "\n",
    "The full schema is available as a text file, which can be downloaded with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24bf9a69",
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 4.6 KiB/4.6 KiB (45.9 KiB/s) with 1 file(s) remaining\r",
      "download: s3://dataincubator-course/spark-stats-data/stack_exchange_schema.txt to ./stack_exchange_schema.txt\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://dataincubator-course/spark-stats-data/stack_exchange_schema.txt ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1d886f",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "You can either get the data by running the appropriate S3 commands in the terminal, or by running this block for the smaller stats data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a6bf1b0",
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "!mkdir -p spark-stats-data\n",
    "!aws s3 sync --exclude '*' --include 'all*' s3://dataincubator-course/spark-stats-data/ ./spark-stats-data\n",
    "!aws s3 sync --exclude '*' --include 'posts*zip' s3://dataincubator-course/spark-stats-data/ ./spark-stats-data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0f4c36",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "And to get the much larger full data set (be warned, this can take 20 or more minutes, so you may want to run it in the terminal to avoid locking up the notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09bb2fb4",
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "!mkdir -p spark-stack-data\n",
    "!aws s3 sync --exclude '*' --include 'all*' s3://dataincubator-course/spark-stack-data/ ./spark-stack-data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0efcb68",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Data input and parsing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4232d89f",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Some rows are split across multiple lines; these can be discarded. Incorrectly formatted XML can also be ignored. It is enough to simply skip problematic rows, the loss of data will not significantly impact our results on these large data sets.\n",
    "\n",
    "**You will need to handle XML parsing yourself.  Our solution uses `lxml.etree` in Python, and we would recommend using this tool yourself to handle the XML parsing.**\n",
    "\n",
    "The goal should be to have a parsing function that can be applied to the input data to access any desired XML elements. You might find it convenient to represent the post, votes, users, etc. data using [`namedtuples`](https://docs.python.org/3/library/collections.html?highlight=namedtuple#collections.namedtuple)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb35c468",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb230a9d",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "This miniproject is divided into two parts, called `spark_data` and `spark_ml`. The first part is doing data analysis in spark, on both a small data set and a large one. This consists of the first six questions in the notebook. The second part is using Spark ML to do machine learning, and is the last two questions. They are distinguished both by sections in the notebook and the question names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a815810b",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Spark data section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47cd6f6",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 1: Bad XML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa3616b",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "This first question is a simple question to test your parsing code. Create an RDD of Post objects where each Post is a valid row of XML from the small \"Cross Validated\" (stats.stackexchange.com) `allPosts` data set.\n",
    "\n",
    "We are going to take several shortcuts to speed up and simplify our computations.  First, your parsing function should only attempt to parse rows that start with `<row` as these denote actual data entries. This should be done in Spark as the data is being read in from disk, without any pre-Spark processing. \n",
    "\n",
    "Return the total number of XML rows that started with `<row` that were subsequently **rejected** during your XML processing.  Note that the text is Unicode, and contains non-ASCII characters.  You may need to re-encode to UTF-8 (depending on your XML parser).\n",
    "\n",
    "**Note that this cleaned data set will be used for questions 1-5.**  (For questions 6-8, you want to similarly remove improperly formatted XML from that data before proceeding further.)  \n",
    "\n",
    "*Question*: Can you figure out what filters you need to put in place to avoid throwing parsing errors entirely?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2044fc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "# create the connection to Spark cluster\n",
    "\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f0bfcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f6d7b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an RDD named 'lines' \n",
    "# RDD is Resilient distributed dataset, it's immutable distributed collection of objects that can be processed in parallel\n",
    "# 'spark-stats-data/allPosts/' contains multiple text files, each line from all the files will form individual elements \n",
    "\n",
    "lines = sc.textFile('spark-stats-data/allPosts/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6341171",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<?xml version=\"1.0\" encoding=\"UTF-8\"?>',\n",
       " '<parent>',\n",
       " '  <row Body=\"\" CommentCount=\"0\" CreationDate=\"2013-10-28T10:42:29.940\" Id=\"73933\" LastActivityDate=\"2013-10-28T10:42:29.940\" LastEditDate=\"2013-10-28T10:42:29.940\" LastEditorUserId=\"686\" OwnerUserId=\"686\" PostTypeId=\"5\" Score=\"0\" />',\n",
       " '  ',\n",
       " '  <row Body=\"See `continuous-data`\" CommentCount=\"0\" CreationDate=\"2013-10-28T10:42:29.940\" Id=\"73934\" LastActivityDate=\"2013-10-28T10:42:29.940\" LastEditDate=\"2013-10-28T10:42:29.940\" LastEditorUserId=\"686\" OwnerUserId=\"686\" PostTypeId=\"4\" Score=\"0\" />']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve the first 5 strings from an RDD and return them in a list\n",
    "\n",
    "lines.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed2f7283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212990"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of strings in 'lines' RDD\n",
    "# total count of strings = 212,990\n",
    "\n",
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ee68edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['  <row Body=\"\" CommentCount=\"0\" CreationDate=\"2013-10-28T10:42:29.940\" Id=\"73933\" LastActivityDate=\"2013-10-28T10:42:29.940\" LastEditDate=\"2013-10-28T10:42:29.940\" LastEditorUserId=\"686\" OwnerUserId=\"686\" PostTypeId=\"5\" Score=\"0\" />',\n",
       " '  <row Body=\"See `continuous-data`\" CommentCount=\"0\" CreationDate=\"2013-10-28T10:42:29.940\" Id=\"73934\" LastActivityDate=\"2013-10-28T10:42:29.940\" LastEditDate=\"2013-10-28T10:42:29.940\" LastEditorUserId=\"686\" OwnerUserId=\"686\" PostTypeId=\"4\" Score=\"0\" />',\n",
       " '  <row Body=\"&lt;p&gt;O.K., I think I found a way of doing it with the correct assumptions. Although it is only useful for my particular problem, maybe somebody can tell me if I am being too &quot;sloppy&quot;, correct me or maybe my approach might be useful to somebody in the future.&lt;/p&gt;&#10;&#10;&lt;p&gt;First of all, I had not realized that the &quot;triangles&quot; account for &quot;long  independent events&quot;. This means that $P(X_i = 0 | \\\\sum_{j\\\\neq i, |j-i| &amp;lt; D} X_j = 1) = 1$ in my notation ($X_i$ represents that an event starts in the moment $i$). What I started doing was a smoothing of $2D$ of $p_i$, so I took the average (can I do this?) in windows. This gave me a way of seeing how many events are there in a sequence.&#10;&lt;img src=&quot;http://i.imgur.com/YGlFKP0.png&quot; alt=&quot;The sequence pi&quot;&gt;&lt;/p&gt;&#10;&#10;&lt;p&gt;The orange line is the original $p_i$ sequence, the black one is the smoothed. After this, in a window of length $2K$, I count how many peaks are there (how many possible events), and the probability of each event is the sum of probabilities from the beginning until the end of the &quot;hill&quot;, although, as can be seen in the picture, sometimes they can overlap, but I have no idea of how can I take that into account. Then, the probability of no event happening in the window of length $2K$ is the product of probabilities of no &quot;long event&quot; happening in this window.&lt;/p&gt;&#10;&#10;&lt;p&gt;Do you think it is a good answer? Do you have any comment/suggestion? Thank you very much.&lt;/p&gt;&#10;\" CommentCount=\"0\" CreationDate=\"2013-10-28T10:49:42.880\" Id=\"73936\" LastActivityDate=\"2013-10-28T10:49:42.880\" OwnerUserId=\"28322\" ParentId=\"73931\" PostTypeId=\"2\" Score=\"0\" />',\n",
       " '  <row Body=\"&lt;p&gt;First, we\\'ll need to know whether you are interested in the response to each Likert question or to a sum of Likert questions; if the latter, it matters how many questions and what the distribution of the scale looks like.&lt;/p&gt;&#10;&#10;&lt;p&gt;Either way, you will have to account for the nonindependence of the data, because the same people are answering the questions multiple times. Repeated measures ANOVA is one solution to this, but it makes unrealistic assumptions including sphericity, and would only be usable for the scale score, and only if the scores ranged fairly widely so that you could pretend they were continuous.&lt;/p&gt;&#10;&#10;&lt;p&gt;A better option is a mixed model. If you treat the scores as continuous data, then this would be a linear mixed model; if you treat them as ordinal (as you would have to do if you were interested in each question) then you would need a nonlinear mixed model.&lt;/p&gt;&#10;&#10;&lt;p&gt;Unfortunately, these models are not simple to implement. If you currently know only about t-tests, then you may need to hire a consultant to help. &lt;/p&gt;&#10;\" CommentCount=\"0\" CreationDate=\"2013-10-28T10:53:25.943\" Id=\"73937\" LastActivityDate=\"2013-10-28T10:53:25.943\" OwnerUserId=\"686\" ParentId=\"65970\" PostTypeId=\"2\" Score=\"1\" />',\n",
       " '  <row AnswerCount=\"1\" Body=\"&lt;p&gt;I have two variables which represent two performance measures.&lt;br&gt;&#10;I ranked a finite set of elements according these two variables.&lt;br&gt;&#10;Therefore, I have to ranks. Suppose the ranks are performed in descending order (the highest the measure the highest the value in the decision process of each element).&lt;/p&gt;&#10;&#10;&lt;p&gt;For instance, an ordered set $\\\\Omega$ of $10$ elements according the two measures A and B.&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;R#A = [5 3 1 9 2 10 6 7 4 8];  &#10;R#B = [9 7 4 8 5 6 1 3 2 10];&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;Suppose that now I truncate R#A and R#B in order to select the &quot;top 5&quot; elements:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;R#A_5 = [5 3 1 9 2];&#10;R#B_5 = [9 7 4 8 5];&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;In your opinion it is still possible to get the Spearman\\'s rho correlation coefficient with these two partial orders?&lt;/p&gt;&#10;&#10;&lt;p&gt;I know that&lt;br&gt;&#10;1) We are in the second step of the Spearman\\'s because we are alrady dealing with ranks.&lt;br&gt;&#10;2) The Sample size is very low but it is just for explanation. &lt;/p&gt;&#10;\" CommentCount=\"0\" CreationDate=\"2013-10-28T11:21:57.617\" Id=\"73938\" LastActivityDate=\"2014-11-24T21:25:49.137\" LastEditDate=\"2013-10-28T23:32:10.697\" LastEditorUserId=\"805\" OwnerUserId=\"9047\" PostTypeId=\"1\" Score=\"0\" Tags=\"&lt;ranking&gt;&lt;rank-correlation&gt;&lt;ranks&gt;&lt;spearman-rho&gt;\" Title=\"Spearman\\'s Rho - from partial ranked variables\" ViewCount=\"88\" />',\n",
       " '  <row AcceptedAnswerId=\"76871\" AnswerCount=\"3\" Body=\"&lt;p&gt;I\\'ve got some temporal data taken from a data logger that I\\'m trying to plot in a graphical form (as a line graph). Because it\\'s a large amount of data, plotting it one one big graph (e.g. in Excel) makes it difficult to explore the visualised data as you can\\'t really zoom in and scroll through the data. What I\\'m looking for is some standalone software that can plot the data as a line graph, but also allow the user to easily scroll through the graph along the horizontal (time) axis and be able to zoom that axis in and out. Ideally, the software would be free and be GUI driven. Does anyone know of any such software?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks,&lt;/p&gt;&#10;\" CommentCount=\"2\" CreationDate=\"2013-10-28T11:29:09.030\" Id=\"73939\" LastActivityDate=\"2015-01-05T19:48:19.133\" LastEditDate=\"2013-10-28T16:16:42.387\" LastEditorUserId=\"31989\" OwnerUserId=\"31989\" PostTypeId=\"1\" Score=\"1\" Tags=\"&lt;data-visualization&gt;\" Title=\"(Standalone) Software for plotting graphs of large amounts of data and allowing you to scroll/zoom\" ViewCount=\"867\" />',\n",
       " '  <row AnswerCount=\"1\" Body=\"&lt;p&gt;Also posted &lt;a href=&quot;http://mathoverflow.net/questions/86936/minimum-distance-estimation-of-mixed-mixture-distributions&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;http://math.stackexchange.com/questions/459747/how-to-mix-probability-estimators-of-the-same-phenomenon&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#10;&#10;&lt;p&gt;I have the following problem:&lt;/p&gt;&#10;&#10;&lt;p&gt;I have N models that give me an estimation of the probability distribution function p(x) of a certain phenomenon x. Let\\'s call them: $p_1(x),...,p_N(x)$. They come from different sources of information, so they can give different values, but they all refere to the same observable fact. Is there a formal and valid way to combine them into a single formula?&lt;/p&gt;&#10;&#10;&lt;p&gt;I have read about mixture distributions, are they applicable to my case?&lt;/p&gt;&#10;&#10;&lt;p&gt;Suppose that my models can be more or less reliable, depending on the source of information they are based on, can I also combine them giving more weight to one model rather than another?&lt;/p&gt;&#10;&#10;&lt;p&gt;One possible solution I have thought is to make a weighted average of all the PDFs: $p(x) = w_1*p_1(x) + ... + w_N * p_N(x)$ where $\\\\sum\\\\limits_{i=0}^N w_i = 1$, does it make sense?&lt;/p&gt;&#10;&#10;&lt;p&gt;Thanks very much for your suggestions!&lt;/p&gt;&#10;&#10;&lt;p&gt;PS: to be more concrete my models give me the probability distribution that a certain person is in the position (x,y) and they rely on different sources of information like the power of a received signal or some other observable fact.&lt;/p&gt;&#10;\" CommentCount=\"0\" CreationDate=\"2013-10-28T11:59:45.803\" Id=\"73941\" LastActivityDate=\"2013-11-28T21:40:04.823\" OwnerUserId=\"31990\" PostTypeId=\"1\" Score=\"1\" Tags=\"&lt;probability&gt;&lt;mixture&gt;\" Title=\"How to mix probability estimators of the same phenomenon?\" ViewCount=\"39\" />',\n",
       " '  <row Body=\"&lt;p&gt;I think you have one of two problems, depending on the what exactly R#A and R#B are. For example, does the &lt;code&gt;5&lt;/code&gt; in R#A mean that the first element has a rank of 5, or does it mean that the fifth element has a rank of 1?&lt;/p&gt;&#10;&#10;&lt;p&gt;If it is the former, then you have not selected the top 5 elements. If it is the latter then R#A and R#B contain different elements.&lt;/p&gt;&#10;&#10;&lt;p&gt;You can certainly run a correlation on the two vectors; but what will the output mean?&lt;/p&gt;&#10;&#10;&lt;p&gt;Perhaps you can tell us what you are trying to accomplish. &lt;/p&gt;&#10;\" CommentCount=\"3\" CreationDate=\"2013-10-28T12:25:34.520\" Id=\"73942\" LastActivityDate=\"2013-10-28T12:25:34.520\" OwnerUserId=\"686\" ParentId=\"73938\" PostTypeId=\"2\" Score=\"1\" />',\n",
       " '  <row AnswerCount=\"1\" Body=\"&lt;p&gt;I have 1750 proteins  that I want to compare the expression level of them between 3 groups (cell-type) using R. How can I do it?&lt;/p&gt;&#10;\" CommentCount=\"2\" CreationDate=\"2013-10-28T12:27:32.710\" Id=\"73943\" LastActivityDate=\"2013-10-28T13:22:00.490\" LastEditDate=\"2013-10-28T12:54:28.473\" LastEditorUserId=\"31992\" OwnerUserId=\"31992\" PostTypeId=\"1\" Score=\"-1\" Tags=\"&lt;r&gt;&lt;bioinformatics&gt;\" Title=\"How can to compare 1750 samples between 3 groups by R?\" ViewCount=\"96\" />',\n",
       " '  <row AcceptedAnswerId=\"73945\" AnswerCount=\"1\" Body=\"&lt;p&gt;Kernel methods are very effective in many supervised classification tasks. So what are the limitations of kernel methods and when to use kernel methods? Especially in the large scale data era, what are the advances of kernel methods? What is the difference between kernel methods and multiple instance learning?&#10;If the data is &lt;code&gt;500x10000&lt;/code&gt;, &lt;code&gt;500&lt;/code&gt; is the count of samples, and &lt;code&gt;10000&lt;/code&gt; is the dimension of each feature, then in this circumstance, can we use the kernel methods?&lt;/p&gt;&#10;\" CommentCount=\"0\" CreationDate=\"2013-10-28T12:33:15.317\" FavoriteCount=\"2\" Id=\"73944\" LastActivityDate=\"2013-10-28T13:04:02.017\" OwnerUserId=\"30834\" PostTypeId=\"1\" Score=\"4\" Tags=\"&lt;machine-learning&gt;&lt;kernel&gt;\" Title=\"What are the limitations of Kernel methods and when to use kernel methods?\" ViewCount=\"501\" />',\n",
       " '  <row Body=\"&lt;p&gt;Kernel methods can be used for supervised and unsupervised problems. Well-known examples are the &lt;a href=&quot;http://en.wikipedia.org/wiki/Support_vector_machine&quot;&gt;support vector machine&lt;/a&gt; and &lt;a href=&quot;https://www.google.be/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=5&amp;amp;cad=rja&amp;amp;ved=0CGQQFjAE&amp;amp;url=http://www.esat.kuleuven.be/sista/lssvmlab/iccha4.pdf&amp;amp;ei=xF5uUqTOJOXd4QSRjYGYAw&amp;amp;usg=AFQjCNGR9oQRvyL9KC2_Og4JT2APGxFxvw&amp;amp;sig2=PsiFFW1EFi6T-g4R0OOzCw&amp;amp;bvm=bv.55123115,d.bGE&quot;&gt;kernel spectral clustering&lt;/a&gt;, respectively.&lt;/p&gt;&#10;&#10;&lt;p&gt;Kernel methods provide a structured way to use a linear algorithm in a transformed feature space, for which the transformation is typically nonlinear (and to a higher dimensional space). The key advantage this so-called kernel trick brings is that nonlinear patterns can be found at a &lt;em&gt;reasonable&lt;/em&gt; computational cost. &lt;/p&gt;&#10;&#10;&lt;p&gt;Note that I said the computational cost is reasonable, but not negligible. Kernel methods typically construct a kernel matrix $\\\\mathbf{K} \\\\in \\\\mathbb{R}^{N\\\\times N}$ with $N$ the number of training instances. The complexity of kernel methods is therefore a function of the number of training instances, rather than the number of input dimensions. Support vector machines, for example, have a training complexity between $O(N^2)$ and $O(N^3)$. For problems with very large $N$, this complexity is currently prohibitive. &lt;/p&gt;&#10;&#10;&lt;p&gt;This makes kernel methods very interesting from a computational perspective when the number of dimensions is large and the number of samples is relatively low (say, less than 1 million).&lt;/p&gt;&#10;&#10;&lt;p&gt;Related: &lt;a href=&quot;http://stats.stackexchange.com/questions/73032/linear-kernel-and-non-linear-kernel-for-support-vector-machine/73156#73156&quot;&gt;Linear kernel and non-linear kernel for support vector machine?&lt;/a&gt;&lt;/p&gt;&#10;&#10;&lt;h2&gt;SVM for Large Scale Problems&lt;/h2&gt;&#10;&#10;&lt;p&gt;For &lt;em&gt;very&lt;/em&gt; high dimensional problems, such as the &lt;code&gt;10000&lt;/code&gt; dimensions you mention in the question, there is often no need to map to a higher dimensional feature space. The input space is already good enough. For such problems, linear methods are &lt;em&gt;orders of magnitude&lt;/em&gt; faster with almost the same predictive performance. Examples of these methods can be found in &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;LIBLINEAR&lt;/a&gt; or &lt;a href=&quot;http://hunch.net/~vw/&quot;&gt;Vowpal Wabbit&lt;/a&gt;. &lt;/p&gt;&#10;&#10;&lt;p&gt;Linear methods are particularly interesting when you have many samples in a high dimensional input space. When you have only $500$ samples, using a nonlinear kernel method will also be cheap (since $N$ is small). If you had, say, $5.000.000$ samples in $10.000$ dimensions, kernel methods would be infeasible.&lt;/p&gt;&#10;&#10;&lt;p&gt;For low-dimensional problems with many training instances (so-called large $N$ small $p$ problems), linear methods may yield poor predictive accuracy. For such problems, ensemble methods such as &lt;a href=&quot;http://esat.kuleuven.be/sista/ensemblesvm&quot;&gt;EnsembleSVM&lt;/a&gt; provide nonlinear decision boundaries at significantly reduced computational cost compared to standard SVM.&lt;/p&gt;&#10;\" CommentCount=\"9\" CreationDate=\"2013-10-28T12:57:31.210\" Id=\"73945\" LastActivityDate=\"2013-10-28T13:04:02.017\" LastEditDate=\"2013-10-28T13:04:02.017\" LastEditorUserId=\"25433\" OwnerUserId=\"25433\" ParentId=\"73944\" PostTypeId=\"2\" Score=\"8\" />',\n",
       " '  <row Body=\"&lt;p&gt;see the answers to &lt;a href=&quot;http://stats.stackexchange.com/questions/9751/do-we-need-a-global-test-before-post-hoc-tests?rq=1&quot;&gt;this&lt;/a&gt; question as well for the relationship between global tests and post-hoc tests.&lt;/p&gt;&#10;\" CommentCount=\"0\" CreationDate=\"2013-10-28T13:00:23.777\" Id=\"73946\" LastActivityDate=\"2013-10-28T13:00:23.777\" OwnerUserId=\"30395\" ParentId=\"29248\" PostTypeId=\"2\" Score=\"0\" />',\n",
       " '  <row Body=\"&lt;p&gt;&lt;a href=&quot;http://www.gnuplot.info/index.html&quot; rel=&quot;nofollow&quot;&gt;Gnuplot&lt;/a&gt; is free, open source and highly versatile and what I use and I think it will meet your needs. You can point and click with the mouse to zoom in and out on any part of a graph, and you can even write a script to scroll through the data as if watching a film.&lt;/p&gt;&#10;\" CommentCount=\"1\" CreationDate=\"2013-10-28T13:08:05.197\" Id=\"73947\" LastActivityDate=\"2013-10-28T13:08:05.197\" OwnerUserId=\"226\" ParentId=\"73939\" PostTypeId=\"2\" Score=\"1\" />',\n",
       " '  <row Body=\"&lt;p&gt;I think Bayesian model comparison might be what you are looking for. See for example &lt;a href=&quot;http://www.amazon.de/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738/ref=sr_1_1?ie=UTF8&amp;amp;qid=1382965736&amp;amp;sr=8-1&amp;amp;keywords=bishop&quot; rel=&quot;nofollow&quot;&gt;Bishop&lt;/a&gt;, Chapter 3.4.&#10;Generally speaking, given a set of $N$ models, you choose your weights to correspond to the posterior probability of each model.&#10;$$ p(M_i | D) \\\\propto p(M_i)p(D | M_i) $$&#10;where $p(M_i)$ is the prior of model importance, you can assume this to be uniform, and $D$ is your data. Hence $p(D | M_i)$ is simply the likelihood of model $i$ given data.&lt;/p&gt;&#10;&#10;&lt;p&gt;The predictive probability for a new value $y^*$ and explanatory values $\\\\mathbf{x}$ is then:&#10;$$ p(y^* | \\\\mathbf{x}, D) = \\\\sum_{i=1}^N p(y^*|\\\\mathbf{x},D,M_i)p(M_i | D)$$&#10;where we use the posteriors as weighting between models.&lt;/p&gt;&#10;&#10;&lt;p&gt;That\\'s the theory. Now in practice, unless you happen to be dealing with conjugated probabilities, you won\\'t get a closed form solution for those posteriors and the above is pretty much useless.&lt;/p&gt;&#10;&#10;&lt;p&gt;When people fit a mixture of distributions, they usually use a mixture of Gaussians and in rare occasions a mixture of t distributions. I think the reason is simply that the mixture is fit using the EM algorithm (again, see Bishop) and Gaussians are particularly useful since their posterior is again a Gaussian and you can get all the required updates for the EM algorithm in closed form solution. And when I say &quot;fit&quot;, they don\\'t fit them individually, but learn the best parameters for all mixture components from the data, which is not what you are doing.&lt;/p&gt;&#10;&#10;&lt;p&gt;Don\\'t worry about that for now and simply check whether you can get the posteriors for your model, or whether you don\\'t want to fit your mixture using the EM and some well known distribution, such as the Gaussian, or t distribution in case of many outliers.&lt;/p&gt;&#10;&#10;&lt;p&gt;EDIT (to your comment):&lt;/p&gt;&#10;&#10;&lt;p&gt;So first of all, my notation: $y$ is the quantity you are trying to model, $x$ is a vector of data used to model $y$.&lt;/p&gt;&#10;&#10;&lt;p&gt;The data $D$ is just a tuple of vectors $x$, used to model a tuple of $y$\\'s. (basically think any normal dataset with a dependent variable and multiple independent ones)&lt;/p&gt;&#10;&#10;&lt;p&gt;OK. now, if I understand correctly you are trying to fit a complex distribution and you somehow already know the models that explain the data (perhaps because you know the generating mechanism) so you only need the mixing proportions.&lt;/p&gt;&#10;&#10;&lt;p&gt;You could try the following, which is guaranteed to work for Gaussians, and I don\\'t see why it shouldn\\'t work for other distributions too. (though there is a big caveat here!)&lt;/p&gt;&#10;&#10;&lt;p&gt;Calculate for each point x the likelihood that the point was generated by model $f1$ and $f2$, where the parameters are known. You end up with a matrix of likelihoods $2 x N$ where N is the number of your points. sum each row and divide by N. You should get the responsibility of each of the models for generating the data.&lt;/p&gt;&#10;&#10;&lt;p&gt;Use that to weight your mixture. You should also check if the resulting density integrates to 1, and if not normalize appropriately.&lt;/p&gt;&#10;\" CommentCount=\"1\" CreationDate=\"2013-10-28T13:20:37.797\" Id=\"73948\" LastActivityDate=\"2013-10-29T19:07:47.983\" LastEditDate=\"2013-10-29T19:07:47.983\" LastEditorUserId=\"12436\" OwnerUserId=\"12436\" ParentId=\"73941\" PostTypeId=\"2\" Score=\"1\" />',\n",
       " '  <row Body=\"&lt;p&gt;Bioconductor project produces software (add-ons for R) for bioinformatics. Bioconductor offers several solutions to your problem. Possibly the easiest one is to use the limma package (&lt;a href=&quot;http://www.bioconductor.org/packages/release/bioc/html/limma.html&quot; rel=&quot;nofollow&quot;&gt;http://www.bioconductor.org/packages/release/bioc/html/limma.html&lt;/a&gt;). It has an extensive user guide that walks you through the basics of the analysis.&lt;/p&gt;&#10;&#10;&lt;p&gt;In addition, see the answer &lt;a href=&quot;http://stats.stackexchange.com/questions/66600/ebayes-lmfit&quot;&gt;eBayes() lmFit()&lt;/a&gt; for a quick overview of the workflow and the functions in the limma package.&lt;/p&gt;&#10;&#10;&lt;p&gt;What you need to consider is the question you are interested in. These questions are coded in a design matrix. For example, if you have one control group (C), and two treatment groups (T1, T2), and you are interested in comparing both treatments with the control group, you might generate the following model matrix. But, before generating the model matrix, let\\'s assume your data matrix (containing the expression values) contains the controls in the three first columns, then three T1 columns, and last three T2 columns. A vector (listing the groups in the same order they appear in the data matrix) containing the group information can be turned into a model matrix as follows:&lt;/p&gt;&#10;&#10;&lt;pre&gt;&lt;code&gt;# &quot;group&quot; vector&#10;groups&amp;lt;-c(&quot;C&quot;,&quot;C&quot;,&quot;C&quot;,&quot;T1&quot;,&quot;T1&quot;,&quot;T1&quot;,&quot;T2&quot;,&quot;T2&quot;,&quot;T2&quot;)&#10;design&amp;lt;-model.matrix(~groups)&#10;design&#10;  (Intercept) groupsT1 groupsT2&#10;1           1        0        0&#10;2           1        0        0&#10;3           1        0        0&#10;4           1        1        0&#10;5           1        1        0&#10;6           1        1        0&#10;7           1        0        1&#10;8           1        0        1&#10;9           1        0        1&#10;&lt;/code&gt;&lt;/pre&gt;&#10;&#10;&lt;p&gt;If you do not have a control group, but some comparison group anyhow, just specify it as the first group (alphabetically) in the groups vector. The model matrix will automatically use it as a baseline with which all others are compared.&lt;/p&gt;&#10;&#10;&lt;p&gt;This does a simple comparison of groups (T1 v. C and T2 v. C). If you have something more complex in mind, please elaborate your question a bit to address this.&lt;/p&gt;&#10;\" CommentCount=\"0\" CreationDate=\"2013-10-28T13:22:00.490\" Id=\"73949\" LastActivityDate=\"2013-10-28T13:22:00.490\" OwnerUserId=\"26206\" ParentId=\"73943\" PostTypeId=\"2\" Score=\"0\" />']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter the 'lines' RDD by choosing only the ones that start with '<row'\n",
    "# strings that start with '<row' = 109,522\n",
    "\n",
    "filtered = lines.filter(lambda line: line.strip().startswith('<row'))\n",
    "print(filtered.count())\n",
    "filtered.take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "303e475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from collections import namedtuple\n",
    "\n",
    "record_tuple = namedtuple('record_tuple', ['FavoriteCount','Score'])\n",
    "\n",
    "def parse(line):\n",
    "    \n",
    "    \"\"\"parse every string to check if it's a valid xml using etree\"\"\"\n",
    "    \n",
    "    try:\n",
    "        tree = ET.fromstring(line.encode('utf-8'))\n",
    "        favoriteCount = int(tree.attrib.get('FavoriteCount', 0))\n",
    "        score = int(tree.attrib.get('Score'))\n",
    "        return record_tuple(favoriteCount, score)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e71ebfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109522"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply (map) 'parse' function to 'filtered' RDD's\n",
    "# the output is the 'xmls' RDD, that has both valid and bad xmls \n",
    "# bad xmls will return 'None'\n",
    "\n",
    "xmls = filtered.map(parse)\n",
    "xmls.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fec41004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(xmls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9954d488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108741"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter xmls by choosing valid ones (the ones that pass parse function successfully)\n",
    "\n",
    "valid_xmls = xmls.filter(lambda x: x is not None)\n",
    "valid_xmls.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6fc88603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108741"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# second way to do the valid counts in one step\n",
    "\n",
    "valid = filtered.map(parse) \\\n",
    "       .filter(lambda x: x is not None)\n",
    "valid.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2893afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "781"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter xmls by choosing bad ones (the ones that don't pass parse function successfully)\n",
    "\n",
    "bad_xmls = xmls.filter(lambda x: x is None)\n",
    "bad_xmls.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94518147",
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 1.0000\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "bad_xml = 781\n",
    "\n",
    "grader.score('spark_data__bad_xml', bad_xml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5088b6a8",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 2: Favorites and scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f00933f",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We're interested in looking for useful patterns in the data.  If we look at the Post data again (the smaller set, `stats.stackexchange.com`), we see that many things about each post are recorded.  We're going to start by looking to see if there is a relationship between the number of times a post was favorited (the `FavoriteCount`) and the `Score`.  The score is the number of times the post was upvoted minus the number of times it was downvoted, so it is a measure of how much a post was liked.  We'd expect posts with a higher number of favorites to have better scores, since they're both measurements of how good the post is.\n",
    "\n",
    "Let's aggregate posts by the number of favorites, and find the average score for each number of favorites.  Do this for the lowest 50 numbers of favorites.\n",
    "\n",
    "**If any field in the Posts or Users is missing, such as the `FavoriteCount`, you should assume it is zero. _Make this assumption for all questions going forward._**\n",
    "\n",
    "_Note:_ Before submitting, take a look at the numbers.  Do they follow the trend you expect?\n",
    "\n",
    "**Checkpoints**\n",
    "\n",
    "- Total score across all posts: 299469\n",
    "- Mean of first 50 favorite counts (averaging the keys themselves): 24.76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b8f223d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[record_tuple(FavoriteCount=0, Score=0),\n",
       " record_tuple(FavoriteCount=0, Score=0),\n",
       " record_tuple(FavoriteCount=0, Score=0),\n",
       " record_tuple(FavoriteCount=0, Score=1),\n",
       " record_tuple(FavoriteCount=0, Score=0),\n",
       " record_tuple(FavoriteCount=0, Score=1),\n",
       " record_tuple(FavoriteCount=0, Score=1),\n",
       " record_tuple(FavoriteCount=0, Score=1),\n",
       " record_tuple(FavoriteCount=0, Score=-1),\n",
       " record_tuple(FavoriteCount=2, Score=4)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the first 10 tuples returned from 'parse' function\n",
    "\n",
    "valid_xmls.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f8d75c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108741"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double check the number of valid_xmls\n",
    "\n",
    "valid_xmls.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64545712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299469"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the total scores across all valid posts (posts that have valid_xmls)\n",
    "\n",
    "total_scores = valid_xmls.map(lambda x: x.Score) \\\n",
    "              .reduce(lambda x, y: x + y) \n",
    "               \n",
    "total_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4412bc06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19, 60, 41, 20, 58, 69, 61, 8, 83, 138]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the number of unique FavoriteCounts, and the randomly output 10 of them\n",
    "\n",
    "unique_favoritecounts = valid_xmls.map(lambda x: x.FavoriteCount).distinct()\n",
    "                                      \n",
    "unique_favoritecounts.takeSample(False, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b0a089a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, (219956, 94003)),\n",
       " (11, (1034, 59)),\n",
       " (22, (422, 13)),\n",
       " (33, (221, 6)),\n",
       " (55, (68, 1))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the sum of scores and the number of scores as a tuple for every distinct FavoriteScount\n",
    "\n",
    "scores_sum_count = valid_xmls.map(lambda x: (x.FavoriteCount, (x.Score, 1))) \\\n",
    "                             .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "scores_sum_count.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec06d844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2.3398827696988396),\n",
       " (11, 17.52542372881356),\n",
       " (22, 32.46153846153846),\n",
       " (33, 36.833333333333336),\n",
       " (55, 68.0),\n",
       " (66, 66.0),\n",
       " (88, 75.0),\n",
       " (275, 222.0),\n",
       " (44, 76.0),\n",
       " (1, 2.7334613999279624)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the average of scores for every distinct FavoriteCount by dividing sum of scores by scores count per FavoriteCount\n",
    "\n",
    "avg_scores = valid_xmls.map(lambda x: (x.FavoriteCount, (x.Score, 1))) \\\n",
    "                  .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])) \\\n",
    "                  .map(lambda x: (x[0], x[1][0] / x[1][1])) \n",
    "\n",
    "avg_scores.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "feaf1603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2.3398827696988396),\n",
       " (1, 2.7334613999279624),\n",
       " (2, 4.481914893617021),\n",
       " (3, 6.350249584026622),\n",
       " (4, 7.656934306569343),\n",
       " (5, 8.941888619854721),\n",
       " (6, 11.263779527559056),\n",
       " (7, 12.916666666666666),\n",
       " (8, 13.345864661654135),\n",
       " (9, 15.754237288135593)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort by FavoriteCount (key) the tuples in ascending order\n",
    "\n",
    "sorted_avg_scores = valid_xmls.map(lambda x: (x.FavoriteCount, (x.Score, 1))) \\\n",
    "                   .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])) \\\n",
    "                   .map(lambda x: (x[0], x[1][0] / x[1][1])) \\\n",
    "                   .sortByKey()\n",
    "\n",
    "sorted_avg_scores.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8809adef",
   "metadata": {},
   "outputs": [],
   "source": [
    "favorite_score = sorted_avg_scores.take(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "afa43121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57.31172468957298"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tried to test the mean of average scores, didn't average the key itslef\n",
    "\n",
    "first_50_avg_scores = avg_scores.take(50)\n",
    "sum_of_averages = sum(value for _, value in first_50_avg_scores)\n",
    "mean_of_averages = sum_of_averages / len(first_50_avg_scores)\n",
    "mean_of_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "67a304f1",
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 1.0000\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "#favorite_score = [(0, 2.3398827696988396)]*50\n",
    "\n",
    "grader.score('spark_data__favorite_score', favorite_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398856b9",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 3: Answer percentage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3061487f",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Investigate the correlation between a user's reputation and the kind of posts they make. For the 99 users with the highest reputation, single out posts which are either questions or answers and look at the percentage of these posts that are answers: *(answers / (answers + questions))*. \n",
    "\n",
    "Return a tuple of their **user ID** and this fraction.\n",
    "\n",
    "You should also return (-1, fraction) to represent the case where you average over all users (so you will return 100 entries total).\n",
    "\n",
    "Again, you only need to run this on the statistics overflow set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bba8777",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "* Total questions: 52,060\n",
    "* Total answers: 55,304\n",
    "* Top 99 users' average reputation: 11893.464646464647"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "689316d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<?xml version=\"1.0\" encoding=\"UTF-8\"?>',\n",
       " '<parent>',\n",
       " '  <row AccountId=\"5872878\" CreationDate=\"2015-03-02T18:42:20.510\" DisplayName=\"Lars Reeker\" DownVotes=\"0\" Id=\"70185\" LastAccessDate=\"2015-03-02T18:42:20.510\" ProfileImageUrl=\"https://lh3.googleusercontent.com/-Y7GNsydm-mc/AAAAAAAAAAI/AAAAAAAADq8/15o5t99O5IU/photo.jpg\" Reputation=\"1\" UpVotes=\"0\" Views=\"0\" />',\n",
       " '  ',\n",
       " '  <row AccountId=\"5872995\" CreationDate=\"2015-03-02T19:04:13.380\" DisplayName=\"Vra\" DownVotes=\"0\" Id=\"70186\" LastAccessDate=\"2015-03-06T15:45:57.590\" Reputation=\"6\" UpVotes=\"0\" Views=\"1\" />']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = sc.textFile('spark-stats-data/allUsers/')\n",
    "users.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20f40c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100425"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "68089efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50458\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['  <row AccountId=\"5872878\" CreationDate=\"2015-03-02T18:42:20.510\" DisplayName=\"Lars Reeker\" DownVotes=\"0\" Id=\"70185\" LastAccessDate=\"2015-03-02T18:42:20.510\" ProfileImageUrl=\"https://lh3.googleusercontent.com/-Y7GNsydm-mc/AAAAAAAAAAI/AAAAAAAADq8/15o5t99O5IU/photo.jpg\" Reputation=\"1\" UpVotes=\"0\" Views=\"0\" />',\n",
       " '  <row AccountId=\"5872995\" CreationDate=\"2015-03-02T19:04:13.380\" DisplayName=\"Vra\" DownVotes=\"0\" Id=\"70186\" LastAccessDate=\"2015-03-06T15:45:57.590\" Reputation=\"6\" UpVotes=\"0\" Views=\"1\" />',\n",
       " '  <row AboutMe=\"\" AccountId=\"5873177\" CreationDate=\"2015-03-02T19:40:16.420\" DisplayName=\"Aroona\" DownVotes=\"0\" Id=\"70187\" LastAccessDate=\"2015-03-02T19:40:16.420\" ProfileImageUrl=\"https://www.gravatar.com/avatar/e0e90702da3203e069f0a7d957ee7ea6?s=128&amp;d=identicon&amp;r=PG&amp;f=1\" Reputation=\"1\" UpVotes=\"0\" Views=\"0\" WebsiteUrl=\"\" />',\n",
       " '  <row AccountId=\"5873184\" CreationDate=\"2015-03-02T19:46:45.400\" DisplayName=\"Yazeed\" DownVotes=\"0\" Id=\"70188\" LastAccessDate=\"2015-03-02T19:46:45.400\" ProfileImageUrl=\"https://www.gravatar.com/avatar/f5e666cb769dfbc30e53bfe8a702dd38?s=128&amp;d=identicon&amp;r=PG&amp;f=1\" Reputation=\"1\" UpVotes=\"0\" Views=\"0\" />',\n",
       " '  <row AccountId=\"228681\" CreationDate=\"2015-03-02T19:56:37.233\" DisplayName=\"Taimur\" DownVotes=\"0\" Id=\"70189\" LastAccessDate=\"2015-03-03T09:26:04.020\" Location=\"London, United Kingdom\" ProfileImageUrl=\"http://i.stack.imgur.com/PhYFp.jpg?s=128&amp;g=1\" Reputation=\"101\" UpVotes=\"0\" Views=\"0\" WebsiteUrl=\"http://curate.im\" />']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_users = users.filter(lambda line: line.strip().startswith('<row'))\n",
    "print(filtered_users.count())\n",
    "filtered_users.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "207fc5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from collections import namedtuple\n",
    "\n",
    "users_tuple = namedtuple('users_tuple', ['Id', 'Reputation'])\n",
    "\n",
    "def parse_users(line):\n",
    "    \n",
    "    \"\"\"parse every string to check if it's a valid xml using etree\"\"\"\n",
    "    \n",
    "    try:\n",
    "        tree = ET.fromstring(line.encode('utf-8'))\n",
    "        user_id = int(tree.attrib.get('Id'))\n",
    "        reputation = int(tree.attrib.get('Reputation'))\n",
    "        return users_tuple(user_id, reputation)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "42f5bb99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50458"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_users = filtered_users.map(parse_users)\n",
    "all_users.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "096ba3b7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[users_tuple(Id=70185, Reputation=1),\n",
       " users_tuple(Id=70186, Reputation=6),\n",
       " users_tuple(Id=70187, Reputation=1),\n",
       " users_tuple(Id=70188, Reputation=1),\n",
       " users_tuple(Id=70189, Reputation=101)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_users = all_users.filter(lambda x: x is not None)\n",
    "print(valid_users.count())\n",
    "valid_users.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "92d24acd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[users_tuple(Id=919, Reputation=100976),\n",
       " users_tuple(Id=805, Reputation=92624),\n",
       " users_tuple(Id=686, Reputation=47334),\n",
       " users_tuple(Id=7290, Reputation=46907),\n",
       " users_tuple(Id=930, Reputation=32283)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do the sorting by reputation at the end\n",
    "\n",
    "sorted_valid_users = valid_users.sortBy(lambda x: x.Reputation, ascending=False)\n",
    "sorted_valid_users.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4d5de68a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11893.464646464647"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpoint #3 - average_reputation: 11,893.46464646\n",
    "\n",
    "top_99_users = sorted_valid_users.take(99)\n",
    "\n",
    "total_reputation_sum = sum(named_tuple.Reputation for named_tuple in top_99_users)\n",
    "average_reputation = total_reputation_sum / len(top_99_users)\n",
    "average_reputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3a769ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = sorted_valid_users.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f5c46e31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total users count: 50320\n",
      "+-----+----------+\n",
      "|   Id|Reputation|\n",
      "+-----+----------+\n",
      "|  919|    100976|\n",
      "|  805|     92624|\n",
      "|  686|     47334|\n",
      "| 7290|     46907|\n",
      "|  930|     32283|\n",
      "| 4505|     27599|\n",
      "| 4253|     25406|\n",
      "|  183|     23610|\n",
      "|11032|     23102|\n",
      "|28746|     22706|\n",
      "|  887|     20315|\n",
      "|  159|     20133|\n",
      "| 2116|     19312|\n",
      "| 4856|     18866|\n",
      "|22047|     17719|\n",
      "| 5739|     16854|\n",
      "| 3277|     16131|\n",
      "|   88|     14768|\n",
      "| 2970|     14500|\n",
      "|  601|     14100|\n",
      "+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"total users count: {users_df.count()}\")\n",
    "users_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ecf82e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "+-----+----------+\n",
      "|   Id|Reputation|\n",
      "+-----+----------+\n",
      "|  919|    100976|\n",
      "|  805|     92624|\n",
      "|  686|     47334|\n",
      "| 7290|     46907|\n",
      "|  930|     32283|\n",
      "| 4505|     27599|\n",
      "| 4253|     25406|\n",
      "|  183|     23610|\n",
      "|11032|     23102|\n",
      "|28746|     22706|\n",
      "|  887|     20315|\n",
      "|  159|     20133|\n",
      "| 2116|     19312|\n",
      "| 4856|     18866|\n",
      "|22047|     17719|\n",
      "| 5739|     16854|\n",
      "| 3277|     16131|\n",
      "|   88|     14768|\n",
      "| 2970|     14500|\n",
      "|  601|     14100|\n",
      "+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_99_df = users_df.limit(99)\n",
    "print(top_99_df.count())\n",
    "top_99_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e60c1ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from collections import namedtuple\n",
    "\n",
    "posts_tuple = namedtuple('posts_tuple', ['OwnerUserId','PostTypeId'])\n",
    "\n",
    "def parse_posts(line):\n",
    "    \n",
    "    \"\"\"parse every string to check if it's a valid xml using etree\"\"\"\n",
    "    \n",
    "    try:\n",
    "        tree = ET.fromstring(line.encode('utf-8'))      \n",
    "        user_id = int(tree.attrib.get('OwnerUserId', 0))    # some of the ownerUserId's are missing\n",
    "        post_type = int(tree.attrib.get('PostTypeId'))\n",
    "        return posts_tuple(user_id, post_type)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "45cb2f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109522"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map 'parse_users' function to 'filtered' posts\n",
    "\n",
    "all_posts = filtered.map(parse_posts)\n",
    "all_posts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1845bf58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108741"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter the valid posts from all posts\n",
    "\n",
    "#valid_posts = all_posts.filter(lambda x: x is not None)\n",
    "#print(valid_posts.count())\n",
    "#valid_posts.take(10)\n",
    "\n",
    "valid_posts = all_posts.filter(lambda x: x is not None)\n",
    "valid_posts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b1024506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52060"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpoint#1 - number of total questions: 52,060\n",
    "\n",
    "valid_posts.filter(lambda x: x.PostTypeId == 1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cfd8bd47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55304"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpoint 2 - number of total answers: 55,304\n",
    "\n",
    "valid_posts.filter(lambda x: x.PostTypeId == 2).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "05eaf078",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107364\n",
      "26888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[posts_tuple(OwnerUserId=28322, PostTypeId=2),\n",
       " posts_tuple(OwnerUserId=686, PostTypeId=2),\n",
       " posts_tuple(OwnerUserId=9047, PostTypeId=1),\n",
       " posts_tuple(OwnerUserId=31989, PostTypeId=1),\n",
       " posts_tuple(OwnerUserId=31990, PostTypeId=1)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the posts that are either questions (1) or answers (2)\n",
    "\n",
    "filtered_valid_posts = valid_posts.filter(lambda x: x[1] in [1,2])\n",
    "\n",
    "print(filtered_valid_posts.count())\n",
    "print(filtered_valid_posts.map(lambda x: x[0]).distinct().count())\n",
    "filtered_valid_posts.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "78948496",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total posts count: 107364\n",
      "total unique users count: 26888\n",
      "+-----------+----------+\n",
      "|OwnerUserId|PostTypeId|\n",
      "+-----------+----------+\n",
      "|      28322|         2|\n",
      "|        686|         2|\n",
      "|       9047|         1|\n",
      "|      31989|         1|\n",
      "|      31990|         1|\n",
      "|        686|         2|\n",
      "|      31992|         1|\n",
      "|      30834|         1|\n",
      "|      25433|         2|\n",
      "|      30395|         2|\n",
      "|        226|         2|\n",
      "|      12436|         2|\n",
      "|      26206|         2|\n",
      "|      29851|         1|\n",
      "|          0|         2|\n",
      "|      30951|         1|\n",
      "|      32000|         1|\n",
      "|      17475|         2|\n",
      "|      19815|         2|\n",
      "|      30557|         1|\n",
      "+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "posts_df = filtered_valid_posts.toDF()\n",
    "print(f\"total posts count: {posts_df.count()}\")\n",
    "print(f\"total unique users count: {posts_df.select('OwnerUserId').distinct().count()}\")\n",
    "posts_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "988002a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|   Id|PostTypeId|\n",
      "+-----+----------+\n",
      "|28322|         2|\n",
      "|  686|         2|\n",
      "| 9047|         1|\n",
      "|31989|         1|\n",
      "|31990|         1|\n",
      "|  686|         2|\n",
      "|31992|         1|\n",
      "|30834|         1|\n",
      "|25433|         2|\n",
      "|30395|         2|\n",
      "|  226|         2|\n",
      "|12436|         2|\n",
      "|26206|         2|\n",
      "|29851|         1|\n",
      "|    0|         2|\n",
      "|30951|         1|\n",
      "|32000|         1|\n",
      "|17475|         2|\n",
      "|19815|         2|\n",
      "|30557|         1|\n",
      "+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "posts_df_renamed = posts_df.selectExpr(\"OwnerUserId as Id\", 'PostTypeId')\n",
    "posts_df_renamed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "62eb8704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27247\n",
      "99\n",
      "+---+----------+----------+\n",
      "| Id|Reputation|PostTypeId|\n",
      "+---+----------+----------+\n",
      "|442|      6588|         2|\n",
      "|442|      6588|         2|\n",
      "|442|      6588|         2|\n",
      "|442|      6588|         2|\n",
      "|442|      6588|         1|\n",
      "|442|      6588|         2|\n",
      "|442|      6588|         2|\n",
      "|442|      6588|         2|\n",
      "|442|      6588|         2|\n",
      "|442|      6588|         2|\n",
      "|442|      6588|         2|\n",
      "|442|      6588|         2|\n",
      "|442|      6588|         2|\n",
      "|442|      6588|         2|\n",
      "|442|      6588|         2|\n",
      "|442|      6588|         2|\n",
      "|442|      6588|         2|\n",
      "|442|      6588|         2|\n",
      "|442|      6588|         2|\n",
      "|442|      6588|         2|\n",
      "+---+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "postsByUser_df = top_99_df.join(posts_df_renamed, 'Id', 'left').select(top_99_df.Id, top_99_df.Reputation, posts_df_renamed.PostTypeId)\n",
    "postsByUser_df.orderBy(\"Reputation\")\n",
    "\n",
    "print(postsByUser_df.count())\n",
    "print(postsByUser_df.select('Id').distinct().count())\n",
    "postsByUser_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "18cd06a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "+-----+-------+-------+------------------+\n",
      "|   Id|count_1|count_2|             ratio|\n",
      "+-----+-------+-------+------------------+\n",
      "|  442|     17|    115|0.8712121212121212|\n",
      "|28666|     18|    162|               0.9|\n",
      "| 3382|      0|    495|               1.0|\n",
      "| 7555|      0|    234|               1.0|\n",
      "|  196|     51|    142|0.7357512953367875|\n",
      "| 2116|      6|    354|0.9833333333333333|\n",
      "| 7224|      4|    161|0.9757575757575757|\n",
      "|  264|     12|    116|           0.90625|\n",
      "| 1108|      1|     35|0.9722222222222222|\n",
      "| 9394|      7|    227|0.9700854700854701|\n",
      "|   25|     13|    143|0.9166666666666666|\n",
      "| 7828|      7|    461|0.9850427350427351|\n",
      "|13047|      4|    146|0.9733333333333334|\n",
      "|  686|     31|   1543|0.9803049555273189|\n",
      "|25433|      3|    223|0.9867256637168141|\n",
      "| 1934|      3|     91|0.9680851063829787|\n",
      "|35989|      8|    148|0.9487179487179487|\n",
      "|32036|      1|    248|0.9959839357429718|\n",
      "| 1739|      1|    194|0.9948717948717949|\n",
      "|  279|      0|    161|               1.0|\n",
      "+-----+-------+-------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# group by 'Id' and calculate the counts of 1s and 2s\n",
    "grouped_df = postsByUser_df.groupBy('Id').agg(\n",
    "    F.sum(F.when(F.col('PostTypeId') == 1, 1).otherwise(0)).alias('count_1'),\n",
    "    F.sum(F.when(F.col('PostTypeId') == 2, 1).otherwise(0)).alias('count_2')\n",
    ")\n",
    "\n",
    "# calculate the ratio of counts of 2 by the sum of counts of 1 and 2\n",
    "result_df = grouped_df.withColumn('ratio', F.col('count_2') / (F.col('count_1') + F.col('count_2')))\n",
    "\n",
    "print(result_df.select('Id').distinct().count())\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e4baf497",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rdd = result_df.rdd\n",
    "b = rdd.map(lambda x: (x[0], x[-1]))\n",
    "\n",
    "\n",
    "# from instructions (-1, answers/ [answers+questions] )\n",
    "\n",
    "last_tuple = (-1, 0.51510748481)\n",
    "\n",
    "\n",
    "top_100 = b.union(sc.parallelize([last_tuple]))\n",
    "\n",
    "answer_percentage = top_100.take(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7abc009",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "#### Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7e76f255",
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 1.0000\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "#answer_percentage = [(7071, 0.9107142857142857)] * 100\n",
    "\n",
    "grader.score('spark_data__answer_percentage', answer_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37eb5716",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 4: First question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1c654f",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We'd expect the first **question** a user asks to be indicative of their future behavior.  We'll dig more into that in the next problem, but for now let's see the relationship between reputation and how long it took each person to ask their first question.\n",
    "\n",
    "For each user that asked a question, find the difference between when their account was created (`CreationDate` for the User) and when they asked their first question (`CreationDate` for their first question).  Return this time difference in days (round down, so 2.7 days counts as 2 days) for the 100 users with the highest reputation, in the form\n",
    "\n",
    "`(UserId, Days)`\n",
    "\n",
    "**Checkpoints**\n",
    "- Users that asked a question: 23134\n",
    "- Average number of days (round each user's days, then average): 30.1074258"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c292d5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from collections import namedtuple\n",
    "\n",
    "q4_tuple = namedtuple('q4_users_tuple', ['Id', 'CreationDate', 'Reputation'])\n",
    "\n",
    "def q4_parser(line):\n",
    "    \n",
    "    \"\"\"parse every string to check if it's a valid xml using etree and get the necessary tag values\"\"\"\n",
    "    \n",
    "    try:\n",
    "        tree = ET.fromstring(line.encode('utf-8'))\n",
    "        user_id = int(tree.attrib.get('Id'))\n",
    "        date = tree.attrib.get('CreationDate')\n",
    "        reputation = int(tree.attrib.get('Reputation'))\n",
    "        return q4_tuple(user_id, date, reputation)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cbdd19a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50458"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q4_users = filtered_users.map(q4_parser)\n",
    "q4_users.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7c9a0da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[q4_users_tuple(Id=70185, CreationDate='2015-03-02T18:42:20.510', Reputation=1),\n",
       " q4_users_tuple(Id=70186, CreationDate='2015-03-02T19:04:13.380', Reputation=6),\n",
       " q4_users_tuple(Id=70187, CreationDate='2015-03-02T19:40:16.420', Reputation=1),\n",
       " q4_users_tuple(Id=70188, CreationDate='2015-03-02T19:46:45.400', Reputation=1),\n",
       " q4_users_tuple(Id=70189, CreationDate='2015-03-02T19:56:37.233', Reputation=101)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q4_valid_users = q4_users.filter(lambda x: x is not None)\n",
    "print(q4_valid_users.count())\n",
    "q4_valid_users.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ff273293",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50320\n",
      "+-----+--------------------+----------+\n",
      "|   Id|        CreationDate|Reputation|\n",
      "+-----+--------------------+----------+\n",
      "|70185|2015-03-02T18:42:...|         1|\n",
      "|70186|2015-03-02T19:04:...|         6|\n",
      "|70187|2015-03-02T19:40:...|         1|\n",
      "|70188|2015-03-02T19:46:...|         1|\n",
      "|70189|2015-03-02T19:56:...|       101|\n",
      "|70190|2015-03-02T19:59:...|         1|\n",
      "|70191|2015-03-02T20:08:...|         1|\n",
      "|70192|2015-03-02T20:10:...|         1|\n",
      "|70193|2015-03-02T20:41:...|         1|\n",
      "|70194|2015-03-02T20:46:...|        11|\n",
      "|70195|2015-03-02T20:52:...|         1|\n",
      "|70196|2015-03-02T20:57:...|         6|\n",
      "|70197|2015-03-02T21:08:...|        18|\n",
      "|70198|2015-03-02T21:37:...|         1|\n",
      "|70199|2015-03-02T21:38:...|        51|\n",
      "|70200|2015-03-02T22:35:...|        11|\n",
      "|70201|2015-03-02T22:39:...|         1|\n",
      "|70202|2015-03-02T23:33:...|       101|\n",
      "|70203|2015-03-02T23:41:...|       116|\n",
      "|70204|2015-03-02T23:45:...|       103|\n",
      "+-----+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert 'q4_valid_users' RDD to 'valid_users_df' dataframe, that has 50,320 unique users with their reputation\n",
    "\n",
    "valid_users_df = q4_valid_users.toDF()\n",
    "\n",
    "print(valid_users_df.select('Id').distinct().count())\n",
    "valid_users_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d52c5422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from collections import namedtuple\n",
    "\n",
    "q4_tuple2 = namedtuple('q4_posts_tuple', ['Id', 'PostDate', 'PostTypeId'])\n",
    "\n",
    "def q4_parser2(line):\n",
    "    \n",
    "    \"\"\"parse every string to check if it's a valid xml using etree and get the necessary tag values\"\"\"\n",
    "    \n",
    "    try:\n",
    "        tree = ET.fromstring(line.encode('utf-8'))      \n",
    "        user_id = int(tree.attrib.get('OwnerUserId', 0))    # some of the ownerUserId's are missing\n",
    "        date = tree.attrib.get('CreationDate')\n",
    "        post_type = int(tree.attrib.get('PostTypeId'))\n",
    "        return q4_tuple2(user_id, date, post_type)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0ef7eb92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109522"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map 'q4_parser2' function to 'filtered' posts\n",
    "\n",
    "q4_posts = filtered.map(q4_parser2)\n",
    "q4_posts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f3354a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108741"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter the valid posts from q4_posts\n",
    "\n",
    "q4_valid_posts = q4_posts.filter(lambda x: x is not None)\n",
    "q4_valid_posts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d418996e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52060"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter the posts by PostTypeId to get just the questions\n",
    "# total number of valid questions: 52,060\n",
    "\n",
    "valid_questions = q4_valid_posts.filter(lambda x: x.PostTypeId == 1)\n",
    "valid_questions.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9476b5a9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23134\n",
      "+-----+--------------------+----------+\n",
      "|   Id|            PostDate|PostTypeId|\n",
      "+-----+--------------------+----------+\n",
      "| 9047|2013-10-28T11:21:...|         1|\n",
      "|31989|2013-10-28T11:29:...|         1|\n",
      "|31990|2013-10-28T11:59:...|         1|\n",
      "|31992|2013-10-28T12:27:...|         1|\n",
      "|30834|2013-10-28T12:33:...|         1|\n",
      "|29851|2013-10-28T14:10:...|         1|\n",
      "|30951|2013-10-28T14:16:...|         1|\n",
      "|32000|2013-10-28T14:55:...|         1|\n",
      "|30557|2013-10-28T15:54:...|         1|\n",
      "|31699|2013-10-28T15:58:...|         1|\n",
      "|17812|2013-10-28T17:06:...|         1|\n",
      "|17812|2013-10-28T17:12:...|         1|\n",
      "|11944|2013-10-28T17:28:...|         1|\n",
      "| 4705|2013-10-28T18:16:...|         1|\n",
      "|32010|2013-10-28T18:36:...|         1|\n",
      "|30523|2013-10-28T20:22:...|         1|\n",
      "|26189|2013-10-28T20:23:...|         1|\n",
      "|21988|2013-10-28T20:33:...|         1|\n",
      "| 4552|2013-10-28T20:58:...|         1|\n",
      "|20227|2013-10-28T21:05:...|         1|\n",
      "+-----+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "valid_questions_df = valid_questions.toDF()\n",
    "print(valid_questions_df.select('Id').distinct().count())         # Checkpoint 1 - Users that asked a question: 23134\n",
    "valid_questions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b5ecea89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23095\n",
      "+-----+----------+--------------------+--------------------+\n",
      "|   Id|Reputation|        CreationDate|   FirstQuestionDate|\n",
      "+-----+----------+--------------------+--------------------+\n",
      "|   26|      3220|2010-07-19T19:09:...|2011-02-18T02:40:...|\n",
      "|   29|      2570|2010-07-19T19:09:...|2010-11-01T20:31:...|\n",
      "|  474|        22|2010-07-27T15:08:...|2010-07-27T15:08:...|\n",
      "| 1950|       430|2010-11-10T17:56:...|2010-11-10T18:13:...|\n",
      "| 2040|       585|2010-11-17T23:45:...|2010-11-23T20:47:...|\n",
      "| 3506|       123|2011-03-02T11:56:...|2011-03-02T12:03:...|\n",
      "| 4823|         6|2011-05-31T20:48:...|2011-05-31T20:57:...|\n",
      "| 5385|        14|2011-07-13T07:55:...|2011-07-13T07:55:...|\n",
      "| 5556|       101|2011-07-26T17:44:...|2011-09-23T14:37:...|\n",
      "| 7225|        13|2011-11-05T08:18:...|2011-11-05T09:10:...|\n",
      "| 7279|        51|2011-11-08T18:30:...|2011-11-08T18:30:...|\n",
      "| 8440|       250|2012-01-10T05:07:...|2013-03-19T22:29:...|\n",
      "| 8484|       116|2012-01-12T07:22:...|2012-01-12T07:29:...|\n",
      "| 9233|      1109|2012-02-15T13:40:...|2012-02-22T12:48:...|\n",
      "| 9458|         2|2012-02-27T10:49:...|2012-02-27T10:49:...|\n",
      "| 9968|        96|2012-03-19T21:35:...|2012-03-19T21:40:...|\n",
      "|11434|        11|2012-05-20T02:45:...|2012-05-20T03:20:...|\n",
      "|11567|        20|2012-05-26T21:50:...|2012-05-26T21:50:...|\n",
      "|11745|        81|2012-06-04T12:26:...|2012-06-06T11:06:...|\n",
      "|13248|         1|2012-08-13T03:19:...|2012-08-13T03:35:...|\n",
      "+-----+----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# join two dataframes by selecting the appropriate columns\n",
    "# group by 'Id' column and aggregate by choosing the earliest QuestionDate (simply need to choose the min) \n",
    "\n",
    "questionsByUser_df = valid_users_df.join(valid_questions_df, 'Id', 'inner') \\\n",
    "                    .select(valid_users_df.Id, valid_users_df.Reputation, valid_users_df.CreationDate, valid_questions_df.PostDate) \\\n",
    "                    .groupBy(\"Id\", \"Reputation\", \"CreationDate\").agg(F.min(\"PostDate\").alias(\"FirstQuestionDate\"))\n",
    "\n",
    "print(questionsByUser_df.select('Id').count())\n",
    "questionsByUser_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0db693aa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23095\n",
      "+-----+----------+--------------------+--------------------+--------------------+--------------------+--------+\n",
      "|   Id|Reputation|        CreationDate|   FirstQuestionDate|               Date1|               Date2|DateDiff|\n",
      "+-----+----------+--------------------+--------------------+--------------------+--------------------+--------+\n",
      "|  919|    100976|2010-08-13T15:29:...|2010-08-17T13:10:...|2010-08-13 15:29:...|2010-08-17 13:10:...|       3|\n",
      "|  805|     92624|2010-08-07T08:40:...|2012-06-07T00:14:...|2010-08-07 08:40:...|2012-06-07 00:14:...|     669|\n",
      "|  686|     47334|2010-08-03T19:42:...|2011-02-10T15:35:...|2010-08-03 19:42:...|2011-02-10 15:35:...|     190|\n",
      "| 7290|     46907|2011-11-09T04:43:...|2011-11-09T05:51:...|2011-11-09 04:43:...|2011-11-09 05:51:...|       0|\n",
      "|  930|     32283|2010-08-13T20:50:...|2010-08-18T20:36:...|2010-08-13 20:50:...|2010-08-18 20:36:...|       4|\n",
      "| 4253|     25406|2011-04-20T12:59:...|2012-04-05T21:54:...|2011-04-20 12:59:...|2012-04-05 21:54:...|     351|\n",
      "|  183|     23610|2010-07-20T02:56:...|2010-07-20T03:31:...|2010-07-20 02:56:...|2010-07-20 03:31:...|       0|\n",
      "|11032|     23102|2012-05-02T14:04:...|2012-05-03T17:04:...|2012-05-02 14:04:...|2012-05-03 17:04:...|       1|\n",
      "|28746|     22706|2013-08-02T14:24:...|2013-08-21T23:19:...|2013-08-02 14:24:...|2013-08-21 23:19:...|      19|\n",
      "|  887|     20315|2010-08-12T07:45:...|2010-09-03T18:23:...|2010-08-12 07:45:...|2010-09-03 18:23:...|      22|\n",
      "|  159|     20133|2010-07-19T23:05:...|2010-07-20T00:15:...|2010-07-19 23:05:...|2010-07-20 00:15:...|       0|\n",
      "| 2116|     19312|2010-11-24T09:52:...|2011-05-30T14:18:...|2010-11-24 09:52:...|2011-05-30 14:18:...|     187|\n",
      "| 4856|     18866|2011-06-02T17:45:...|2011-07-14T18:11:...|2011-06-02 17:45:...|2011-07-14 18:11:...|      42|\n",
      "| 5739|     16854|2011-08-08T17:31:...|2012-01-03T15:13:...|2011-08-08 17:31:...|2012-01-03 15:13:...|     147|\n",
      "| 3277|     16131|2011-02-16T19:33:...|2011-02-16T19:33:...|2011-02-16 19:33:...|2011-02-16 19:33:...|       0|\n",
      "|   88|     14768|2010-07-19T19:35:...|2010-07-20T18:12:...|2010-07-19 19:35:...|2010-07-20 18:12:...|       0|\n",
      "|  601|     14100|2010-07-29T14:29:...|2010-08-17T19:15:...|2010-07-29 14:29:...|2010-08-17 19:15:...|      19|\n",
      "|17230|     13557|2012-11-27T12:24:...|2013-05-19T13:31:...|2012-11-27 12:24:...|2013-05-19 13:31:...|     173|\n",
      "| 2392|     12491|2010-12-15T12:27:...|2011-01-27T03:57:...|2010-12-15 12:27:...|2011-01-27 03:57:...|      42|\n",
      "| 1390|     12098|2010-09-21T18:58:...|2011-01-17T15:44:...|2010-09-21 18:58:...|2011-01-17 15:44:...|     117|\n",
      "+-----+----------+--------------------+--------------------+--------------------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, asc,desc, ceil\n",
    "import math\n",
    "from pyspark.sql.functions import floor, col\n",
    "\n",
    "# convert date strings using timestamp, then find the date difference in days by rounding it down\n",
    "# finally sort the table based on 'Reputation' column\n",
    "\n",
    "updated_df = questionsByUser_df.withColumn(\"Date1\", F.to_timestamp(F.col(\"CreationDate\"))) \\\n",
    "                               .withColumn(\"Date2\", F.to_timestamp(F.col(\"FirstQuestionDate\"))) \\\n",
    "                               .withColumn(\"DateDiff\", floor((F.col(\"Date2\").cast(\"long\") - F.col(\"Date1\").cast(\"long\")) / 86400)) \\\n",
    "                               .sort(desc(\"Reputation\"))\n",
    "\n",
    "print(questionsByUser_df.select('Id').count())       # double check the number of users in a dataframe\n",
    "updated_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "49c99fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|     avg(DateDiff)|\n",
      "+------------------+\n",
      "|30.086165836761204|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# checkpoint 2 - Average number of days (round each user's days, then average): 30.1074258\n",
    "\n",
    "updated_df.select(avg('DateDiff')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8197d5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the spark dataframe back to rdd, \n",
    "# select the first and last columns to make a tuple of the first 100 users with the highest reputation\n",
    "\n",
    "q4_rdd = updated_df.rdd\n",
    "first_question = q4_rdd.map(lambda x: (x[0], x[6])).take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7be33154",
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 1.0000\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "#first_question = [(805, 669)] * 100\n",
    "\n",
    "grader.score('spark_data__first_question', first_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8002f444",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 5: Identify veterans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a5e040",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "It can be interesting to think about what factors influence a user to remain active on the site over a long period of time. In order not to bias the results towards older users, we'll define a time window between 100 and 150 days after account creation. If the user has made a post in this time, we'll consider them active and well on their way to being veterans of the site; if not, they are inactive and were likely brief users.\n",
    "\n",
    "*Consider*: What other parameterizations of \"activity\" could we use, and how would they differ in terms of splitting our user base?\n",
    "\n",
    "*Consider*: What other biases are still not dealt with, after using the above approach?\n",
    "\n",
    "Let's see if there are differences between the first ever question posts of \"veterans\" vs. \"brief users\". For each group separately, average the score, views, number of answers, and number of favorites of the users' **first question**. Remember, if the score, views, answers, or favorites is missing, you should assume it is zero.\n",
    "\n",
    "*Consider*: What story could you tell from these numbers? How do the numbers support it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c581de",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "#### Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b928556",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "* Total brief users: 24,864\n",
    "* Total veteran users: 2,027"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d63084ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------+\n",
      "|   Id|        CreationDate|Reputation|\n",
      "+-----+--------------------+----------+\n",
      "|70185|2015-03-02T18:42:...|         1|\n",
      "|70186|2015-03-02T19:04:...|         6|\n",
      "|70187|2015-03-02T19:40:...|         1|\n",
      "|70188|2015-03-02T19:46:...|         1|\n",
      "|70189|2015-03-02T19:56:...|       101|\n",
      "|70190|2015-03-02T19:59:...|         1|\n",
      "|70191|2015-03-02T20:08:...|         1|\n",
      "|70192|2015-03-02T20:10:...|         1|\n",
      "|70193|2015-03-02T20:41:...|         1|\n",
      "|70194|2015-03-02T20:46:...|        11|\n",
      "|70195|2015-03-02T20:52:...|         1|\n",
      "|70196|2015-03-02T20:57:...|         6|\n",
      "|70197|2015-03-02T21:08:...|        18|\n",
      "|70198|2015-03-02T21:37:...|         1|\n",
      "|70199|2015-03-02T21:38:...|        51|\n",
      "|70200|2015-03-02T22:35:...|        11|\n",
      "|70201|2015-03-02T22:39:...|         1|\n",
      "|70202|2015-03-02T23:33:...|       101|\n",
      "|70203|2015-03-02T23:41:...|       116|\n",
      "|70204|2015-03-02T23:45:...|       103|\n",
      "+-----+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# double check the valid_users_df.\n",
    "# will need to remove 'reputation' column\n",
    "\n",
    "valid_users_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ee2ffe3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50320\n",
      "+-----+--------------------+\n",
      "|   Id|        CreationDate|\n",
      "+-----+--------------------+\n",
      "|70185|2015-03-02T18:42:...|\n",
      "|70186|2015-03-02T19:04:...|\n",
      "|70187|2015-03-02T19:40:...|\n",
      "|70188|2015-03-02T19:46:...|\n",
      "|70189|2015-03-02T19:56:...|\n",
      "|70190|2015-03-02T19:59:...|\n",
      "|70191|2015-03-02T20:08:...|\n",
      "|70192|2015-03-02T20:10:...|\n",
      "|70193|2015-03-02T20:41:...|\n",
      "|70194|2015-03-02T20:46:...|\n",
      "|70195|2015-03-02T20:52:...|\n",
      "|70196|2015-03-02T20:57:...|\n",
      "|70197|2015-03-02T21:08:...|\n",
      "|70198|2015-03-02T21:37:...|\n",
      "|70199|2015-03-02T21:38:...|\n",
      "|70200|2015-03-02T22:35:...|\n",
      "|70201|2015-03-02T22:39:...|\n",
      "|70202|2015-03-02T23:33:...|\n",
      "|70203|2015-03-02T23:41:...|\n",
      "|70204|2015-03-02T23:45:...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# all users with their account creation dates\n",
    "\n",
    "q5_users_df = valid_users_df.drop('Reputation')\n",
    "\n",
    "print(q5_users_df.select('Id').distinct().count())\n",
    "\n",
    "q5_users_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8cc1a4f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108741\n",
      "+-----+--------------------+----------+\n",
      "|   Id|            PostDate|PostTypeId|\n",
      "+-----+--------------------+----------+\n",
      "|  686|2013-10-28T10:42:...|         5|\n",
      "|  686|2013-10-28T10:42:...|         4|\n",
      "|28322|2013-10-28T10:49:...|         2|\n",
      "|  686|2013-10-28T10:53:...|         2|\n",
      "| 9047|2013-10-28T11:21:...|         1|\n",
      "|31989|2013-10-28T11:29:...|         1|\n",
      "|31990|2013-10-28T11:59:...|         1|\n",
      "|  686|2013-10-28T12:25:...|         2|\n",
      "|31992|2013-10-28T12:27:...|         1|\n",
      "|30834|2013-10-28T12:33:...|         1|\n",
      "|25433|2013-10-28T12:57:...|         2|\n",
      "|30395|2013-10-28T13:00:...|         2|\n",
      "|  226|2013-10-28T13:08:...|         2|\n",
      "|12436|2013-10-28T13:20:...|         2|\n",
      "|26206|2013-10-28T13:22:...|         2|\n",
      "|29851|2013-10-28T14:10:...|         1|\n",
      "|    0|2013-10-28T14:12:...|         2|\n",
      "|30951|2013-10-28T14:16:...|         1|\n",
      "|32000|2013-10-28T14:55:...|         1|\n",
      "|17475|2013-10-28T15:03:...|         2|\n",
      "+-----+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# all the posts made by users \n",
    "\n",
    "q5_posts_df = q4_valid_posts.toDF()\n",
    "\n",
    "print(q5_posts_df.select('Id').count())\n",
    "\n",
    "q5_posts_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "00e2c70f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107042\n",
      "+---+--------------------+--------------------+\n",
      "| Id|        CreationDate|            PostDate|\n",
      "+---+--------------------+--------------------+\n",
      "| 26|2010-07-19T19:09:...|2011-05-19T19:02:...|\n",
      "| 26|2010-07-19T19:09:...|2011-05-22T04:08:...|\n",
      "| 26|2010-07-19T19:09:...|2011-05-26T05:12:...|\n",
      "| 26|2010-07-19T19:09:...|2011-05-26T20:15:...|\n",
      "| 26|2010-07-19T19:09:...|2011-05-27T04:59:...|\n",
      "| 26|2010-07-19T19:09:...|2011-05-27T23:07:...|\n",
      "| 26|2010-07-19T19:09:...|2011-05-29T18:11:...|\n",
      "| 26|2010-07-19T19:09:...|2011-06-06T15:03:...|\n",
      "| 26|2010-07-19T19:09:...|2011-06-07T17:04:...|\n",
      "| 26|2010-07-19T19:09:...|2011-06-08T16:06:...|\n",
      "| 26|2010-07-19T19:09:...|2011-06-08T22:37:...|\n",
      "| 26|2010-07-19T19:09:...|2011-06-09T03:39:...|\n",
      "| 26|2010-07-19T19:09:...|2011-06-10T02:12:...|\n",
      "| 26|2010-07-19T19:09:...|2011-06-10T16:21:...|\n",
      "| 26|2010-07-19T19:09:...|2011-06-11T07:13:...|\n",
      "| 26|2010-07-19T19:09:...|2011-06-11T17:50:...|\n",
      "| 26|2010-07-19T19:09:...|2011-06-12T17:03:...|\n",
      "| 26|2010-07-19T19:09:...|2011-06-13T18:36:...|\n",
      "| 26|2010-07-19T19:09:...|2011-06-16T18:25:...|\n",
      "| 26|2010-07-19T19:09:...|2011-06-24T18:18:...|\n",
      "+---+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# total numbers of users in 2 categories: 26891   (veterans + brief)\n",
    "\n",
    "q5_df = q5_users_df.join(q5_posts_df, 'Id', 'inner') \\\n",
    "                   .select(q5_users_df.Id, q5_users_df.CreationDate, q5_posts_df.PostDate) \n",
    "\n",
    "print(q5_df.select('Id').count())\n",
    "q5_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4e7f1dad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107042\n",
      "+---+--------------------+--------------------+--------------------+--------------+\n",
      "| Id|        CreationDate|            PostDate| AccountCreationDate|DaysDifference|\n",
      "+---+--------------------+--------------------+--------------------+--------------+\n",
      "| 26|2010-07-19T19:09:...|2011-05-19 19:02:...|2010-07-19 19:09:...|           303|\n",
      "| 26|2010-07-19T19:09:...|2011-05-22 04:08:...|2010-07-19 19:09:...|           306|\n",
      "| 26|2010-07-19T19:09:...|2011-05-26 05:12:...|2010-07-19 19:09:...|           310|\n",
      "| 26|2010-07-19T19:09:...|2011-05-26 20:15:...|2010-07-19 19:09:...|           311|\n",
      "| 26|2010-07-19T19:09:...|2011-05-27 04:59:...|2010-07-19 19:09:...|           311|\n",
      "| 26|2010-07-19T19:09:...|2011-05-27 23:07:...|2010-07-19 19:09:...|           312|\n",
      "| 26|2010-07-19T19:09:...|2011-05-29 18:11:...|2010-07-19 19:09:...|           313|\n",
      "| 26|2010-07-19T19:09:...|2011-06-06 15:03:...|2010-07-19 19:09:...|           321|\n",
      "| 26|2010-07-19T19:09:...|2011-06-07 17:04:...|2010-07-19 19:09:...|           322|\n",
      "| 26|2010-07-19T19:09:...|2011-06-08 16:06:...|2010-07-19 19:09:...|           323|\n",
      "| 26|2010-07-19T19:09:...|2011-06-08 22:37:...|2010-07-19 19:09:...|           324|\n",
      "| 26|2010-07-19T19:09:...|2011-06-09 03:39:...|2010-07-19 19:09:...|           324|\n",
      "| 26|2010-07-19T19:09:...|2011-06-10 02:12:...|2010-07-19 19:09:...|           325|\n",
      "| 26|2010-07-19T19:09:...|2011-06-10 16:21:...|2010-07-19 19:09:...|           325|\n",
      "| 26|2010-07-19T19:09:...|2011-06-11 07:13:...|2010-07-19 19:09:...|           326|\n",
      "| 26|2010-07-19T19:09:...|2011-06-11 17:50:...|2010-07-19 19:09:...|           326|\n",
      "| 26|2010-07-19T19:09:...|2011-06-12 17:03:...|2010-07-19 19:09:...|           327|\n",
      "| 26|2010-07-19T19:09:...|2011-06-13 18:36:...|2010-07-19 19:09:...|           328|\n",
      "| 26|2010-07-19T19:09:...|2011-06-16 18:25:...|2010-07-19 19:09:...|           331|\n",
      "| 26|2010-07-19T19:09:...|2011-06-24 18:18:...|2010-07-19 19:09:...|           339|\n",
      "+---+--------------------+--------------------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, desc, floor\n",
    "#import math\n",
    "\n",
    "\n",
    "# convert date strings using timestamp, then find the date difference in days by rounding it down\n",
    "# finally sort the table based on 'Reputation' column\n",
    "\n",
    "updated_q5_df = q5_df.withColumn(\"AccountCreationDate\", F.to_timestamp(F.col(\"CreationDate\"))) \\\n",
    "        .withColumn(\"PostDate\", F.to_timestamp(F.col(\"PostDate\"))) \\\n",
    "        .withColumn(\"DaysDifference\", floor((F.col(\"PostDate\").cast(\"long\") - F.col(\"AccountCreationDate\").cast(\"long\")) / 86400))\n",
    "\n",
    "print(updated_q5_df.select('Id').count())       # double check the number of users in a dataframe\n",
    "updated_q5_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d3879520",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+--------------+------+\n",
      "| Id|        CreationDate|            PostDate| AccountCreationDate|DaysDifference|Status|\n",
      "+---+--------------------+--------------------+--------------------+--------------+------+\n",
      "| 26|2010-07-19T19:09:...|2011-05-19 19:02:...|2010-07-19 19:09:...|           303|     0|\n",
      "| 26|2010-07-19T19:09:...|2011-05-22 04:08:...|2010-07-19 19:09:...|           306|     0|\n",
      "| 26|2010-07-19T19:09:...|2011-05-26 05:12:...|2010-07-19 19:09:...|           310|     0|\n",
      "| 26|2010-07-19T19:09:...|2011-05-26 20:15:...|2010-07-19 19:09:...|           311|     0|\n",
      "| 26|2010-07-19T19:09:...|2011-05-27 04:59:...|2010-07-19 19:09:...|           311|     0|\n",
      "| 26|2010-07-19T19:09:...|2011-05-27 23:07:...|2010-07-19 19:09:...|           312|     0|\n",
      "| 26|2010-07-19T19:09:...|2011-05-29 18:11:...|2010-07-19 19:09:...|           313|     0|\n",
      "| 26|2010-07-19T19:09:...|2011-06-06 15:03:...|2010-07-19 19:09:...|           321|     0|\n",
      "| 26|2010-07-19T19:09:...|2011-06-07 17:04:...|2010-07-19 19:09:...|           322|     0|\n",
      "| 26|2010-07-19T19:09:...|2011-06-08 16:06:...|2010-07-19 19:09:...|           323|     0|\n",
      "| 26|2010-07-19T19:09:...|2011-06-08 22:37:...|2010-07-19 19:09:...|           324|     0|\n",
      "| 26|2010-07-19T19:09:...|2011-06-09 03:39:...|2010-07-19 19:09:...|           324|     0|\n",
      "| 26|2010-07-19T19:09:...|2011-06-10 02:12:...|2010-07-19 19:09:...|           325|     0|\n",
      "| 26|2010-07-19T19:09:...|2011-06-10 16:21:...|2010-07-19 19:09:...|           325|     0|\n",
      "| 26|2010-07-19T19:09:...|2011-06-11 07:13:...|2010-07-19 19:09:...|           326|     0|\n",
      "| 26|2010-07-19T19:09:...|2011-06-11 17:50:...|2010-07-19 19:09:...|           326|     0|\n",
      "| 26|2010-07-19T19:09:...|2011-06-12 17:03:...|2010-07-19 19:09:...|           327|     0|\n",
      "| 26|2010-07-19T19:09:...|2011-06-13 18:36:...|2010-07-19 19:09:...|           328|     0|\n",
      "| 26|2010-07-19T19:09:...|2011-06-16 18:25:...|2010-07-19 19:09:...|           331|     0|\n",
      "| 26|2010-07-19T19:09:...|2011-06-24 18:18:...|2010-07-19 19:09:...|           339|     0|\n",
      "+---+--------------------+--------------------+--------------------+--------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a new column 'status' to check if the post date was within 100 and 150 days\n",
    "\n",
    "status_column = F.when((F.col(\"DaysDifference\") >= 100) & (F.col(\"DaysDifference\") < 150), 1).otherwise(0)\n",
    "\n",
    "updated_q5_df2 = updated_q5_df.withColumn(\"Status\", status_column) \n",
    "                             \n",
    "updated_q5_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f0ef19aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26847\n",
      "+-----+-------+\n",
      "|   Id| Result|\n",
      "+-----+-------+\n",
      "|   26|  brief|\n",
      "|   29|veteran|\n",
      "|  474|  brief|\n",
      "| 1806|  brief|\n",
      "| 1950|  brief|\n",
      "| 2040|veteran|\n",
      "| 3506|  brief|\n",
      "| 4823|  brief|\n",
      "| 5385|  brief|\n",
      "| 5556|  brief|\n",
      "| 6721|  brief|\n",
      "| 7225|  brief|\n",
      "| 7279|  brief|\n",
      "| 8440|  brief|\n",
      "| 8484|  brief|\n",
      "| 9233|  brief|\n",
      "| 9458|  brief|\n",
      "| 9968|  brief|\n",
      "|11434|  brief|\n",
      "|11567|  brief|\n",
      "+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classify every user as a 'veteran' or 'brief user' based on their post date\n",
    "\n",
    "grouped_df = updated_q5_df2.groupBy(\"Id\").agg(F.collect_list(\"Status\").alias(\"StatusList\"))\n",
    "df_with_result = grouped_df.withColumn(\"Result\", F.when(F.array_contains(\"StatusList\", 1) == True, 'veteran').otherwise('brief')).drop(\"StatusList\")\n",
    "\n",
    "print(df_with_result.select('Id').distinct().count())\n",
    "df_with_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fd8b4f21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2027\n",
      "+-----+-------+\n",
      "|   Id| Result|\n",
      "+-----+-------+\n",
      "|   29|veteran|\n",
      "| 2040|veteran|\n",
      "|35148|veteran|\n",
      "|39520|veteran|\n",
      "|41988|veteran|\n",
      "|52743|veteran|\n",
      "| 7546|veteran|\n",
      "| 8101|veteran|\n",
      "|15921|veteran|\n",
      "|23772|veteran|\n",
      "|26140|veteran|\n",
      "|27183|veteran|\n",
      "|27276|veteran|\n",
      "|53215|veteran|\n",
      "|53990|veteran|\n",
      "| 6489|veteran|\n",
      "|17814|veteran|\n",
      "|21138|veteran|\n",
      "|21417|veteran|\n",
      "|35243|veteran|\n",
      "+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a separate sub-dataset with veterans only\n",
    "\n",
    "veterans = df_with_result.where(df_with_result.Result=='veteran')\n",
    "\n",
    "print(veterans.select('Id').distinct().count())\n",
    "veterans.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d1f1c478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1818\n",
      "+-----+-------+--------------------+\n",
      "|   Id| Result|        CreationDate|\n",
      "+-----+-------+--------------------+\n",
      "|   29|veteran|2010-11-01T20:31:...|\n",
      "| 2040|veteran|2010-11-23T20:47:...|\n",
      "|35148|veteran|2013-11-21T23:25:...|\n",
      "|41988|veteran|2014-03-15T15:38:...|\n",
      "|52743|veteran|2014-11-29T20:39:...|\n",
      "| 7546|veteran|2011-11-22T18:29:...|\n",
      "| 8101|veteran|2011-06-30T00:19:...|\n",
      "|15921|veteran|2012-10-14T05:42:...|\n",
      "|23772|veteran|2013-04-01T14:39:...|\n",
      "|26140|veteran|2013-05-26T12:02:...|\n",
      "|27276|veteran|2013-06-25T10:49:...|\n",
      "|53215|veteran|2014-12-16T21:30:...|\n",
      "|53990|veteran|2014-08-21T20:24:...|\n",
      "| 6489|veteran|2011-09-24T15:37:...|\n",
      "|17814|veteran|2013-04-10T13:57:...|\n",
      "|21138|veteran|2013-02-22T05:19:...|\n",
      "|21417|veteran|2013-03-21T10:35:...|\n",
      "|35243|veteran|2013-11-24T13:11:...|\n",
      "|41420|veteran|2014-07-08T08:58:...|\n",
      "|42246|veteran|2014-03-20T11:09:...|\n",
      "+-----+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questionsByVeterans_df = veterans.join(questionsByUser_df, 'Id', 'inner') \\\n",
    "                                  .select(veterans.Id, veterans.Result, questionsByUser_df.FirstQuestionDate.alias(\"CreationDate\")) \n",
    "\n",
    "print(questionsByVeterans_df.select('Id').count())\n",
    "questionsByVeterans_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9465ff12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24820\n",
      "+-----+------+\n",
      "|   Id|Result|\n",
      "+-----+------+\n",
      "|   26| brief|\n",
      "|  474| brief|\n",
      "| 1806| brief|\n",
      "| 1950| brief|\n",
      "| 3506| brief|\n",
      "| 4823| brief|\n",
      "| 5385| brief|\n",
      "| 5556| brief|\n",
      "| 6721| brief|\n",
      "| 7225| brief|\n",
      "| 7279| brief|\n",
      "| 8440| brief|\n",
      "| 8484| brief|\n",
      "| 9233| brief|\n",
      "| 9458| brief|\n",
      "| 9968| brief|\n",
      "|11434| brief|\n",
      "|11567| brief|\n",
      "|11625| brief|\n",
      "|11745| brief|\n",
      "+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "briefs = df_with_result.where(df_with_result.Result=='brief')\n",
    "\n",
    "print(briefs.select('Id').distinct().count())\n",
    "briefs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9f941ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21277\n",
      "+-----+------+--------------------+\n",
      "|   Id|Result|        CreationDate|\n",
      "+-----+------+--------------------+\n",
      "|   26| brief|2011-02-18T02:40:...|\n",
      "|  474| brief|2010-07-27T15:08:...|\n",
      "| 1950| brief|2010-11-10T18:13:...|\n",
      "| 3506| brief|2011-03-02T12:03:...|\n",
      "| 4823| brief|2011-05-31T20:57:...|\n",
      "| 5385| brief|2011-07-13T07:55:...|\n",
      "| 5556| brief|2011-09-23T14:37:...|\n",
      "| 7225| brief|2011-11-05T09:10:...|\n",
      "| 7279| brief|2011-11-08T18:30:...|\n",
      "| 8440| brief|2013-03-19T22:29:...|\n",
      "| 8484| brief|2012-01-12T07:29:...|\n",
      "| 9233| brief|2012-02-22T12:48:...|\n",
      "| 9458| brief|2012-02-27T10:49:...|\n",
      "| 9968| brief|2012-03-19T21:40:...|\n",
      "|11434| brief|2012-05-20T03:20:...|\n",
      "|11567| brief|2012-05-26T21:50:...|\n",
      "|11745| brief|2012-06-06T11:06:...|\n",
      "|13248| brief|2012-08-13T03:35:...|\n",
      "|14719| brief|2012-10-07T13:17:...|\n",
      "|14846| brief|2012-10-11T01:31:...|\n",
      "+-----+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questionsByBriefs_df = briefs.join(questionsByUser_df, 'Id', 'inner') \\\n",
    "                                  .select(briefs.Id, briefs.Result, questionsByUser_df.FirstQuestionDate.alias(\"CreationDate\")) \n",
    "\n",
    "print(questionsByBriefs_df.select('Id').count())\n",
    "questionsByBriefs_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d2444951",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brief users count: 24820.\n",
      "veteran users count: 2027.\n"
     ]
    }
   ],
   "source": [
    "# Total brief users: 24,864\n",
    "# Total veteran users: 2,027\n",
    "\n",
    "print(f\"brief users count: {df_with_result.select('Result').where(df_with_result.Result=='brief').count()}.\")\n",
    "print(f\"veteran users count: {df_with_result.select('Result').where(df_with_result.Result=='veteran').count()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9b4c3c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser for question 5 to get the features from 'posts' data\n",
    "\n",
    "q5_tuple = namedtuple('q5_tuple', ['Id', 'Score', 'ViewCount', 'AnswerCount', 'FavoriteCount', 'CreationDate', 'PostTypeId'])\n",
    "\n",
    "def q5_parser(line):\n",
    "    \n",
    "    \"\"\"parse every string to check if it's a valid xml using etree\"\"\"\n",
    "    \n",
    "    try:\n",
    "        tree = ET.fromstring(line.encode('utf-8'))      \n",
    "        user_id = int(tree.attrib.get('OwnerUserId', 0))    # some of the ownerUserId's are missing\n",
    "        score = int(tree.attrib.get('Score', 0))\n",
    "        view = int(tree.attrib.get('ViewCount', 0))\n",
    "        answer = int(tree.attrib.get('AnswerCount', 0))\n",
    "        fav_count = int(tree.attrib.get('FavoriteCount', 0))\n",
    "        date = tree.attrib.get('CreationDate')\n",
    "        post_type = int(tree.attrib.get('PostTypeId'))\n",
    "        return q5_tuple(user_id, score, view, answer, fav_count, date, post_type)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8bfc4e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lines.filter(lambda line: line.strip().startswith('<row'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "43d0702b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108741"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q5_data = f.map(q5_parser) \\\n",
    "           .filter(lambda x: x is not None) \n",
    "\n",
    "q5_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bf1a5935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52060"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = q5_data.filter(lambda x: x.PostTypeId==1)\n",
    "questions.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0a66ce8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all posts: 52060.\n",
      "unique posts: 23134.\n",
      "+-----+-----+---------+-----------+-------------+--------------------+----------+\n",
      "|   Id|Score|ViewCount|AnswerCount|FavoriteCount|        CreationDate|PostTypeId|\n",
      "+-----+-----+---------+-----------+-------------+--------------------+----------+\n",
      "| 9047|    0|       88|          1|            0|2013-10-28T11:21:...|         1|\n",
      "|31989|    1|      867|          3|            0|2013-10-28T11:29:...|         1|\n",
      "|31990|    1|       39|          1|            0|2013-10-28T11:59:...|         1|\n",
      "|31992|   -1|       96|          1|            0|2013-10-28T12:27:...|         1|\n",
      "|30834|    4|      501|          1|            2|2013-10-28T12:33:...|         1|\n",
      "|29851|    1|      217|          0|            0|2013-10-28T14:10:...|         1|\n",
      "|30951|    3|       73|          0|            0|2013-10-28T14:16:...|         1|\n",
      "|32000|    0|       61|          1|            0|2013-10-28T14:55:...|         1|\n",
      "|30557|    1|      258|          1|            0|2013-10-28T15:54:...|         1|\n",
      "|31699|    1|       42|          0|            0|2013-10-28T15:58:...|         1|\n",
      "|17812|    0|       59|          2|            0|2013-10-28T17:06:...|         1|\n",
      "|17812|    1|      131|          3|            0|2013-10-28T17:12:...|         1|\n",
      "|11944|    3|      301|          1|            1|2013-10-28T17:28:...|         1|\n",
      "| 4705|    1|     1770|          2|            0|2013-10-28T18:16:...|         1|\n",
      "|32010|    0|       29|          1|            0|2013-10-28T18:36:...|         1|\n",
      "|30523|    1|       76|          0|            0|2013-10-28T20:22:...|         1|\n",
      "|26189|    4|      187|          1|            1|2013-10-28T20:23:...|         1|\n",
      "|21988|    1|      307|          0|            1|2013-10-28T20:33:...|         1|\n",
      "| 4552|    4|       65|          0|            0|2013-10-28T20:58:...|         1|\n",
      "|20227|    5|      397|          1|            0|2013-10-28T21:05:...|         1|\n",
      "+-----+-----+---------+-----------+-------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q5_data_df = questions.toDF()\n",
    "\n",
    "\n",
    "print(f\"all posts: {q5_data_df.select('Id').count()}.\")\n",
    "print(f\"unique posts: {q5_data_df.select('Id').distinct().count()}.\")\n",
    "\n",
    "q5_data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d0d2ef00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1818\n",
      "+-----+-----+---------+-----------+-------------+\n",
      "|   Id|Score|ViewCount|AnswerCount|FavoriteCount|\n",
      "+-----+-----+---------+-----------+-------------+\n",
      "| 1894|    3|     1216|          1|            0|\n",
      "| 3762|    3|      242|          1|            1|\n",
      "| 8397|    1|     2743|          1|            0|\n",
      "| 8926|    3|      979|          2|            2|\n",
      "| 9577|    5|    20332|          3|            7|\n",
      "|10457|    5|       81|          2|            1|\n",
      "|12885|    1|       88|          2|            0|\n",
      "|17293|    5|      509|          1|            1|\n",
      "|20011|    3|      607|          2|            0|\n",
      "|21639|    3|      579|          1|            1|\n",
      "|29325|    4|     1021|          1|            3|\n",
      "|41737|    0|       76|          0|            0|\n",
      "|   29|    8|      440|          1|            0|\n",
      "| 1679|   12|      906|          1|            4|\n",
      "| 3794|    7|      145|          1|            1|\n",
      "| 8724|    7|      784|          1|            3|\n",
      "| 9311|    4|      268|          2|            0|\n",
      "|12710|    0|      150|          2|            1|\n",
      "|12719|    1|     2264|          1|            0|\n",
      "|21026|    2|       69|          1|            0|\n",
      "+-----+-----+---------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Veterans_data_df = questionsByVeterans_df.join(q5_data_df, ['Id','CreationDate'], 'inner') \\\n",
    "      .select(questionsByVeterans_df.Id, q5_data_df.Score, q5_data_df.ViewCount, q5_data_df.AnswerCount, q5_data_df.FavoriteCount) \n",
    "               \n",
    "\n",
    "print(Veterans_data_df.select('Id').distinct().count())\n",
    "Veterans_data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0a966667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+------------------+------------------+\n",
      "|        avg(Score)|   avg(ViewCount)|  avg(AnswerCount)|avg(FavoriteCount)|\n",
      "+------------------+-----------------+------------------+------------------+\n",
      "|3.5434543454345433|926.3982398239824|1.2981298129812981| 1.300880088008801|\n",
      "+------------------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "veterans_averages = Veterans_data_df.select([avg('Score'), avg('ViewCount'), avg('AnswerCount'), avg('FavoriteCount')])\n",
    "veterans_averages.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "06a0c16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21277\n",
      "+-----+-----+---------+-----------+-------------+\n",
      "|   Id|Score|ViewCount|AnswerCount|FavoriteCount|\n",
      "+-----+-----+---------+-----------+-------------+\n",
      "|   78|    4|      118|          2|            0|\n",
      "|  862|    6|     1773|          3|            3|\n",
      "| 1441|    6|       99|          1|            0|\n",
      "| 2157|    1|      425|          1|            0|\n",
      "| 2516|    3|      525|          1|            0|\n",
      "| 3921|    2|     2563|          3|            2|\n",
      "| 4375|    0|      294|          1|            1|\n",
      "| 4551|    1|      879|          3|            0|\n",
      "| 4591|    1|      163|          1|            1|\n",
      "| 5081|    1|      127|          1|            0|\n",
      "| 5172|    6|     1067|          2|            2|\n",
      "| 5513|    4|      434|          0|            0|\n",
      "| 5727|    4|      356|          1|            0|\n",
      "| 5754|    9|     1688|          2|            3|\n",
      "| 5808|    0|      208|          2|            0|\n",
      "| 7225|    2|      153|          1|            0|\n",
      "| 7359|    2|      369|          2|            1|\n",
      "| 7590|   15|      682|          0|            0|\n",
      "| 8403|    1|      228|          2|            0|\n",
      "|10056|    5|     1140|          3|            4|\n",
      "+-----+-----+---------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Briefs_data_df = questionsByBriefs_df.join(q5_data_df, ['Id','CreationDate'], 'inner') \\\n",
    "      .select(questionsByBriefs_df.Id, q5_data_df.Score, q5_data_df.ViewCount, q5_data_df.AnswerCount, q5_data_df.FavoriteCount) \n",
    "               \n",
    "\n",
    "print(Briefs_data_df.select('Id').distinct().count())\n",
    "Briefs_data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4dd0445e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+------------------+------------------+\n",
      "|        avg(Score)|   avg(ViewCount)|  avg(AnswerCount)|avg(FavoriteCount)|\n",
      "+------------------+-----------------+------------------+------------------+\n",
      "|2.1009023404455305|553.4952533132813|0.9706739355202557|0.5758529937024156|\n",
      "+------------------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "briefs_averages = Briefs_data_df.select([avg('Score'), avg('ViewCount'), avg('AnswerCount'), avg('FavoriteCount')])\n",
    "briefs_averages.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a12ba12e",
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 1.0000\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "identify_veterans = {\n",
    "    \"vet_score\": 3.5434543454345433,\n",
    "    \"vet_views\": 926.3982398239824,\n",
    "    \"vet_answers\": 1.2981298129812981,\n",
    "    \"vet_favorites\": 1.300880088008801,\n",
    "    \"brief_score\": 2.1009023404455305,\n",
    "    \"brief_views\": 553.4952533132813,\n",
    "    \"brief_answers\": 0.9706739355202557,\n",
    "    \"brief_favorites\": 0.5758529937024156\n",
    "}\n",
    "\n",
    "grader.score('spark_data__identify_veterans', identify_veterans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd48944",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 6: Identify veterans&mdash;full\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd648fc1",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Same as above, but on the full Stack Exchange data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28bb2b7",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "#### Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ddbff2",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "* Total brief users: 1,848,628\n",
    "* Total veteran users: 288,285"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "dea267a5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brief users count: 1846117.\n",
      "veteran users count: 288285.\n",
      "+------------------+------------------+------------------+------------------+\n",
      "|        avg(Score)|    avg(ViewCount)|  avg(AnswerCount)|avg(FavoriteCount)|\n",
      "+------------------+------------------+------------------+------------------+\n",
      "|2.2598437331442924|1844.0344896669696|1.8426197044183144|0.8673157237744455|\n",
      "+------------------+------------------+------------------+------------------+\n",
      "\n",
      "+------------------+------------------+------------------+------------------+\n",
      "|        avg(Score)|    avg(ViewCount)|  avg(AnswerCount)|avg(FavoriteCount)|\n",
      "+------------------+------------------+------------------+------------------+\n",
      "|1.1307456144103445|1096.1519220732553|1.5038565525030159|0.3861764445851408|\n",
      "+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import namedtuple\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, desc, floor, avg\n",
    "\n",
    "\n",
    "# create the connection to Spark cluster\n",
    "# sc = pyspark.SparkContext('local[*]')\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "\n",
    "\n",
    "# read the files from directory\n",
    "posts = sc.textFile('spark-stack-data/allPosts/')\n",
    "users = sc.textFile('spark-stack-data/allUsers/')\n",
    "\n",
    "\n",
    "\n",
    "# filter tags\n",
    "filtered_posts = posts.filter(lambda line: line.strip().startswith('<row'))\n",
    "filtered_users = users.filter(lambda line: line.strip().startswith('<row'))\n",
    "\n",
    "\n",
    "\n",
    "user_tuple = namedtuple('user_tuple', ['Id', 'Account_Creation_Date', 'Reputation'])\n",
    "\n",
    "# create a f-n to parse users\n",
    "def parse_user(user): \n",
    "    try:\n",
    "        tree = ET.fromstring(user.encode('utf-8'))\n",
    "        user_id = int(tree.attrib.get('Id'))\n",
    "        date = tree.attrib.get('CreationDate')\n",
    "        reputation = int(tree.attrib.get('Reputation'))\n",
    "        return user_tuple(user_id, date, reputation)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    \n",
    "    \n",
    "# map 'parse_user' function to filtered users and choose the users that were parsed\n",
    "all_users = filtered_users.map(parse_user) \n",
    "valid_users = all_users.filter(lambda x: x is not None)\n",
    "\n",
    "\n",
    "\n",
    "# convert 'valid_users' RDD to spark dataframe 'valid_users_df'\n",
    "valid_users_df = valid_users.toDF()\n",
    "\n",
    "\n",
    "\n",
    "# drop the 'Reputation' column to keep all the users with their 'id's and account 'CreationDates'\n",
    "users_df = valid_users_df.drop('Reputation')\n",
    "\n",
    "\n",
    "\n",
    "post_tuple = namedtuple('post_tuple', ['Id', 'Creation_Date', 'PostTypeId'])\n",
    "\n",
    "# create a f-n to parse posts\n",
    "def parse_post(post):   \n",
    "    try:\n",
    "        tree = ET.fromstring(post.encode('utf-8'))      \n",
    "        user_id = int(tree.attrib.get('OwnerUserId', 0))    # some of the ownerUserId's are missing\n",
    "        date = tree.attrib.get('CreationDate')\n",
    "        post_type = int(tree.attrib.get('PostTypeId'))\n",
    "        return post_tuple(user_id, date, post_type)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    \n",
    "    \n",
    "# map 'parse_post' function to filtered posts and choose the posts that were parsed\n",
    "all_posts = filtered_posts.map(parse_post) \n",
    "valid_posts = all_posts.filter(lambda x: x is not None)\n",
    "\n",
    "\n",
    "\n",
    "# convert 'valid_posts' RDD to spark dataframe 'posts_df'\n",
    "posts_df = valid_posts.toDF()\n",
    "\n",
    "\n",
    "\n",
    "# total numbers of users in 2 categories: 26891   (veterans + brief)\n",
    "joined_df = users_df.join(posts_df, 'Id', 'inner') \\\n",
    "                    .select(users_df.Id, users_df.Account_Creation_Date, posts_df.Creation_Date) \n",
    "\n",
    "\n",
    "\n",
    "# convert date strings using timestamp, then find the date difference in days by rounding it down\n",
    "updated_joined_df = joined_df.withColumn(\"AccountCreationDate\", F.to_timestamp(F.col(\"Account_Creation_Date\"))) \\\n",
    "                             .withColumn(\"CreationDate\", F.to_timestamp(F.col(\"Creation_Date\"))) \\\n",
    "                             .withColumn(\"DaysDifference\", floor((F.col(\"CreationDate\").cast(\"long\") - F.col(\"AccountCreationDate\").cast(\"long\")) / 86400))\n",
    "\n",
    "\n",
    "\n",
    "# create a new column 'status' to check if the post date was within 100 and 150 days\n",
    "status_column = F.when((F.col(\"DaysDifference\") >= 100) & (F.col(\"DaysDifference\") < 150), 1).otherwise(0)\n",
    "updated_joined_df2 = updated_joined_df.withColumn(\"Status\", status_column) \n",
    "\n",
    "\n",
    "\n",
    "# classify every user as a 'veteran' or 'brief user' based on their post date\n",
    "updated_joined_df3 = updated_joined_df2.groupBy(\"Id\").agg(F.collect_list(\"Status\").alias(\"StatusList\"))\n",
    "veterans_briefs_df = updated_joined_df3.withColumn(\"Result\", F.when(F.array_contains(\"StatusList\", 1) == True, 'veteran').otherwise('brief')).drop(\"StatusList\")\n",
    "\n",
    "\n",
    "\n",
    "# create sub-datasets with 'veterans' and 'brief' users separately\n",
    "veterans = veterans_briefs_df.where(veterans_briefs_df.Result=='veteran')\n",
    "briefs = veterans_briefs_df.where(veterans_briefs_df.Result=='brief')\n",
    "\n",
    "\n",
    "\n",
    "# parser for question 5 to get the features from 'posts' data\n",
    "\n",
    "post_tuple2 = namedtuple('post_tuple2',['Id','Score','ViewCount','AnswerCount','FavoriteCount','CreationDate','PostTypeId'])\n",
    "\n",
    "def parse_post2(post):   \n",
    "    try:\n",
    "        tree = ET.fromstring(post.encode('utf-8'))      \n",
    "        user_id = int(tree.attrib.get('OwnerUserId', 0))    # some of the ownerUserId's are missing\n",
    "        score = int(tree.attrib.get('Score', 0))\n",
    "        view = int(tree.attrib.get('ViewCount', 0))\n",
    "        answer = int(tree.attrib.get('AnswerCount', 0))\n",
    "        fav_count = int(tree.attrib.get('FavoriteCount', 0))\n",
    "        date = tree.attrib.get('CreationDate')\n",
    "        post_type = int(tree.attrib.get('PostTypeId'))\n",
    "        return post_tuple2(user_id, score, view, answer, fav_count, date, post_type)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    \n",
    "\n",
    "all_posts2 = filtered_posts.map(parse_post2) \n",
    "valid_posts2 = all_posts2.filter(lambda x: x is not None)    \n",
    "\n",
    "\n",
    "# select only 'questions' among posts\n",
    "questions = valid_posts2.filter(lambda x: x.PostTypeId==1)\n",
    "\n",
    "\n",
    "# convert 'questions' RDD to spark dataframe 'questions_df'\n",
    "questions_df = questions.toDF()\n",
    "\n",
    "\n",
    "\n",
    "questionsByUser_df = users_df.join(questions_df, 'Id', 'inner') \\\n",
    "                    .select(users_df.Id, users_df.Account_Creation_Date, questions_df.CreationDate) \\\n",
    "                    .groupBy(\"Id\", \"Account_Creation_Date\").agg(F.min(\"CreationDate\").alias(\"CreationDate\"))\n",
    "\n",
    "\n",
    "\n",
    "# join two dataframes to get questions by 'veterans'\n",
    "questionsByVeterans_df = veterans.join(questionsByUser_df, 'Id', 'inner') \\\n",
    "                                 .select(veterans.Id, veterans.Result, questionsByUser_df.CreationDate.alias(\"CreationDate\")) \n",
    "\n",
    "\n",
    "\n",
    "# join two dataframes to get questions by 'brief' users\n",
    "questionsByBriefs_df = briefs.join(questionsByUser_df, 'Id', 'inner') \\\n",
    "                             .select(briefs.Id, briefs.Result, questionsByUser_df.CreationDate.alias(\"CreationDate\")) \n",
    "\n",
    "\n",
    "\n",
    "print(f\"brief users count: {veterans_briefs_df.select('Result').where(veterans_briefs_df.Result=='brief').count()}.\")\n",
    "print(f\"veteran users count: {veterans_briefs_df.select('Result').where(veterans_briefs_df.Result=='veteran').count()}.\")\n",
    "\n",
    "\n",
    "\n",
    "Veterans_data_df = questionsByVeterans_df.join(questions_df, ['Id','CreationDate'], 'inner') \\\n",
    "                                         .select(questionsByVeterans_df.Id, questions_df.Score, questions_df.ViewCount, questions_df.AnswerCount, questions_df.FavoriteCount) \n",
    "\n",
    "Briefs_data_df = questionsByBriefs_df.join(questions_df, ['Id','CreationDate'], 'inner') \\\n",
    "                                     .select(questionsByBriefs_df.Id, questions_df.Score, questions_df.ViewCount, questions_df.AnswerCount, questions_df.FavoriteCount) \n",
    "\n",
    "\n",
    "\n",
    "# calculate the averages of the features for both 'veterans' and 'brief' users\n",
    "veterans_averages = Veterans_data_df.select([avg('Score'), avg('ViewCount'), avg('AnswerCount'), avg('FavoriteCount')])    \n",
    "briefs_averages = Briefs_data_df.select([avg('Score'), avg('ViewCount'), avg('AnswerCount'), avg('FavoriteCount')])\n",
    "\n",
    "\n",
    "\n",
    "veterans_averages.show()\n",
    "briefs_averages.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "416d3a0e",
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 1.0000\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "identify_veterans_full = {\n",
    "    \"vet_score\": 2.2598437331442924,\n",
    "    \"vet_views\": 1844.0344896669696,\n",
    "    \"vet_answers\": 1.8426197044183144,\n",
    "    \"vet_favorites\": 0.8673157237744455,\n",
    "    \"brief_score\": 1.1307456144103445,\n",
    "    \"brief_views\": 1096.1519220732553,\n",
    "    \"brief_answers\": 1.5038565525030159,\n",
    "    \"brief_favorites\": 0.3861764445851408\n",
    "}\n",
    "\n",
    "grader.score('spark_data__identify_veterans_full', identify_veterans_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cb42d0",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "This ends the `spark_data` section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0540f6a7",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4cc88e",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Spark ML questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f94513e",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "The questions from here forward are associated with the `spark_ml` prefix. They are working with Spark's ML library to do some NLP based analysis on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db39885",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e753d7e",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 7: Word2vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64533bd0",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Word2Vec is one approach for vectorizing text data. The vectorized representations of words in the vocabulary tend to be useful for predicting other words in the document, hence the famous example \"vector('king') - vector('man') + vector('woman') ~= vector('queen')\".\n",
    "\n",
    "Let's see how good a Word2Vec model we can train using the **tags** of each Stack Exchange post as documents (this uses the full data set). Use the implementation of Word2Vec from Spark ML (this will require using DataFrames) to return a list of the top 25 closest synonyms to \"ggplot2\" and their similarity score in tuple format (\"string\", number).\n",
    "\n",
    "The tags appear in the data as one string, you will need to separate them into individual tags. There is no need to further parse them beyond separating them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984aa39f",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "#### Parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a9fd1c",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "The dimensionality of the vector space should be 100. The random seed should be 42 in `PySpark`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9470a5",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "#### Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777d8ec8",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "* Mean of the top 25 cosine similarities: 0.8012362027168274"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f70acebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "#sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f793f3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import namedtuple\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "posts = sc.textFile('spark-stack-data/allPosts/')\n",
    "\n",
    "\n",
    "pattern = '<([^>]+)>'                                 # regex pattern to capture the words inside tags\n",
    "tag_tuple = namedtuple('tag_tuple', ['Tags'])         # pull out only tags from every line of posts \n",
    "\n",
    "\n",
    "def parse_tag(post):   \n",
    "    try:\n",
    "        tree = ET.fromstring(post.encode('utf-8'))      \n",
    "        tag = tree.attrib.get('Tags')\n",
    "        tag = re.findall(pattern, tag)\n",
    "        return tag_tuple(tag)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0d1626f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter posts by '<row', apply f-n 'parse_tag' to every post, filter out invalid xmls, convert RDD to dataframe\n",
    "\n",
    "tags = posts.filter(lambda line: line.strip().startswith('<row'))\\\n",
    "            .map(parse_tag)\\\n",
    "            .filter(lambda x: x is not None)\\\n",
    "            .toDF(['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "79e524e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                tags|\n",
      "+--------------------+\n",
      "|[types, xsd, jaxb...|\n",
      "|[javascript, jque...|\n",
      "|[android, backgro...|\n",
      "|[animation, monot...|\n",
      "|            [jquery]|\n",
      "|      [xml, ms-word]|\n",
      "|[asp.net-mvc, whi...|\n",
      "|[gis, openstreetm...|\n",
      "|[iphone, cocos2d-...|\n",
      "|[profiling, gprof...|\n",
      "|       [php, arrays]|\n",
      "|      [c#, winforms]|\n",
      "|[ruby-on-rails, r...|\n",
      "|[php, mysql, apache]|\n",
      "|[blackberry, blac...|\n",
      "| [c, multithreading]|\n",
      "|[asp.net-mvc-3, n...|\n",
      "|[soap, protocols,...|\n",
      "|[c#, asp.net, .ne...|\n",
      "|[c, oracle, prepr...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tags.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0d026c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec(inputCol='tags', outputCol='vectors', vectorSize=30, minCount=10, seed=42)\n",
    "model = w2v.fit(tags)\n",
    "result = model.transform(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "87fb4ee4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|                word|        similarity|\n",
      "+--------------------+------------------+\n",
      "|             lattice|0.9696043729782104|\n",
      "|              r-grid|0.9443326592445374|\n",
      "|           levelplot|0.9430872797966003|\n",
      "|             boxplot|0.9414194226264954|\n",
      "|                nlme|0.9279255867004395|\n",
      "| confidence-interval|0.9261690378189087|\n",
      "|               anova|0.9255669713020325|\n",
      "|                  lm|  0.92326819896698|\n",
      "|      standard-error| 0.922639787197113|\n",
      "|             plotrix|0.9209862947463989|\n",
      "|                 zoo|0.9206746220588684|\n",
      "|            quantile|0.9187219738960266|\n",
      "|                 rgl|0.9182008504867554|\n",
      "|                ecdf|0.9143278002738953|\n",
      "|performanceanalytics|0.9127399921417236|\n",
      "|                 xts|0.9124394059181213|\n",
      "|            quantmod| 0.911929190158844|\n",
      "|             p-value|0.9117739796638489|\n",
      "|   survival-analysis|0.9113281965255737|\n",
      "|               loess|0.9109570980072021|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.findSynonyms('ggplot2', 25).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "de199229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[avg(similarity): double]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "#checkpoint - mean of the top 25 cosine similarities: 0.8012362027168274\n",
    "\n",
    "print(model.findSynonyms('ggplot2', 25).select(avg('similarity')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "8a263603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lattice', 0.9696043729782104),\n",
       " ('r-grid', 0.9443326592445374),\n",
       " ('levelplot', 0.9430872797966003),\n",
       " ('boxplot', 0.9414194226264954),\n",
       " ('nlme', 0.9279255867004395),\n",
       " ('confidence-interval', 0.9261690378189087),\n",
       " ('anova', 0.9255669713020325),\n",
       " ('lm', 0.92326819896698),\n",
       " ('standard-error', 0.922639787197113),\n",
       " ('plotrix', 0.9209862947463989),\n",
       " ('zoo', 0.9206746220588684),\n",
       " ('quantile', 0.9187219738960266),\n",
       " ('rgl', 0.9182008504867554),\n",
       " ('ecdf', 0.9143278002738953),\n",
       " ('performanceanalytics', 0.9127399921417236),\n",
       " ('xts', 0.9124394059181213),\n",
       " ('quantmod', 0.911929190158844),\n",
       " ('p-value', 0.9117739796638489),\n",
       " ('survival-analysis', 0.9113281965255737),\n",
       " ('loess', 0.9109570980072021),\n",
       " ('lme4', 0.910408616065979),\n",
       " ('line-plot', 0.9095427989959717),\n",
       " ('plotmath', 0.908098042011261),\n",
       " ('categorical-data', 0.9057424068450928),\n",
       " ('regression', 0.9040448665618896)]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonyms_rdd = model.findSynonyms('ggplot2', 25).rdd        # convert \"model.findSynonyms('ggplot2', 25)\" dataframe to rdd\n",
    "final_rdd = synonyms_rdd.map(lambda x: (x[0], x[1]))        # get the result in the form of tuples of two columns\n",
    "word2vec = final_rdd.take(25)                               # store just those first 25 synonyms into a word2vec\n",
    "word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "591cd968",
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 0.9600\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "# word2vec = [(\"data.frame\", 0.772650957107544)] * 25\n",
    "\n",
    "grader.score('spark_ml__word2vec', word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb67c60",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 8: Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c3f88d",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We'd like to see if we can predict the tags of a **question** from its body text. Instead of predicting specific tags, we will instead try to predict if a question contains one of the top ten most common tags.  \n",
    "\n",
    "To this end, we have separated out a train and a test set from the original data.  The training and tests sets were downloaded with the stats data at the beginning of the notebook.  You can also get them from S3:\n",
    "  * `s3://dataincubator-course/spark-stats-data/posts_train.zip`\n",
    "  * `s3://dataincubator-course/spark-stats-data/posts_test.zip`\n",
    "\n",
    "This will involve two steps: first, find the ten most common tags for questions in the training data set (the tags have been removed from the test set). Then train a learner to predict from the text of the question (the `Body` attribute) if it should have one of those ten tags in it - you will need to process the question text with NLP techniques such as splitting the text into tokens.\n",
    "\n",
    "Since we can't reliably pickle Spark models, instead return a list of your predictions, sorted by the question's `Id`.  This sorting is very important, as our grader expects the results to be submitted in a particular order. These predictions should be `0` if the question isn't expected to have a tag in the top ten, and `1` if it is.\n",
    "\n",
    "As an example, if our top tags include `spark` and `python`, and we had the following questions:\n",
    "\n",
    "```\n",
    "<row Body=\"...\" Id=\"1740\" Tags=\"<machine-learning><spark><regression>\" ... />\n",
    "<row Body=\"...\" Id=\"723\" Tags=\"<statistics><neurons>\" ... />\n",
    "<row Body=\"...\" Id=\"2740\" Tags=\"<functional><python><spark><pyspark>\" ... />\n",
    "```\n",
    "\n",
    "We would expect to return `[0, 1, 1]` (for the order `[723, 1740, 2740]`).  You may need to do some format manipulation in your DataFrame to get this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492b4be9",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "#### Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aac32ab",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "- Number of training posts with a tag in the top 10: `22525`\n",
    "- Number without: `19540`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dbce7ac",
   "metadata": {
    "scrolled": true,
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  spark-stats-data/posts_train.zip\n",
      "Archive:  spark-stats-data/posts_test.zip\n"
     ]
    }
   ],
   "source": [
    "!unzip -u -d spark-stats-data/train spark-stats-data/posts_train.zip\n",
    "!unzip -u -d spark-stats-data/test spark-stats-data/posts_test.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0f3f221",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0092557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from collections import namedtuple\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "import re\n",
    "\n",
    "# read the train data\n",
    "posts = sc.textFile('spark-stats-data/train/')\n",
    "\n",
    "pattern = '<([^>]+)>'            # regex pattern to capture the words inside tags\n",
    "\n",
    "# pull out the necessary information from every post\n",
    "question_tag_tuple = namedtuple('question_tag_tuple', ['Id', 'Tags', 'Body', 'PostTypeId'])         \n",
    "\n",
    "def parse_question_tag(post):   \n",
    "    try:\n",
    "        tree = ET.fromstring(post.encode('utf-8'))   \n",
    "        post_id = int(tree.attrib.get('Id', 0))                   # pull the question id, but not the 'OwnerUesrId'\n",
    "        tag = tree.attrib.get('Tags')\n",
    "        tag = re.findall(pattern, tag)                            # capture only what's within the tags (<> text <>)\n",
    "        body = tree.attrib.get('Body')\n",
    "        body = re.sub(r'<p>|</p>|<i>|</i>|\\n', '', body)          # drop the '<p>' tags\n",
    "        post_type = int(tree.attrib.get('PostTypeId'))\n",
    "        return question_tag_tuple(post_id, tag, body, post_type)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb862d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+----------+\n",
      "|   Id|                Tags|                Body|PostTypeId|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "|10893|[confidence-inter...|I'm developing an...|         1|\n",
      "|10897|       [probability]|I would like to c...|         1|\n",
      "|10900|[distributions, r...|I have to generat...|         1|\n",
      "|10905|[r, mixed-model, ...|I have searched f...|         1|\n",
      "|10907|[regression, basi...|I have data with ...|         1|\n",
      "|10910|      [spss, scales]|I have a 37-quest...|         1|\n",
      "|10918|                 [r]|I'm trying to do ...|         1|\n",
      "|10920|          [variance]|I am looking for ...|         1|\n",
      "|10926|[r, confidence-in...|As question, I ha...|         1|\n",
      "|10928|[probability, hyp...|If i test two hyp...|         1|\n",
      "|10943|[mixed-model, var...|I have a mixed ef...|         1|\n",
      "|10945|[correlation, fac...|If an exploratory...|         1|\n",
      "|10947|[r, sas, autocorr...|I am trying to fi...|         1|\n",
      "|10949|[regression, prob...|I want to estimat...|         1|\n",
      "|10951|[random-variable,...|I have a question...|         1|\n",
      "|10953|[repeated-measure...|I have a data set...|         1|\n",
      "|10958|       [interaction]|I am trying to in...|         1|\n",
      "|10961|    [r, time-series]|I am trying to mo...|         1|\n",
      "|10963|[algorithms, matc...|I have data like ...|         1|\n",
      "|10965|    [r, forecasting]|I'm using the <co...|         1|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# allows to convert RDD to spark dataframe\n",
    "sqlContext = SQLContext(sc) \n",
    "\n",
    "# parse all posts\n",
    "# choose only the posts that pass parser\n",
    "# choose the questions only\n",
    "# select the following columns\n",
    "\n",
    "question_tags = posts.filter(lambda line: line.strip().startswith('<row'))\n",
    "filtered_question_tags = question_tags.map(parse_question_tag)    \n",
    "mapped_question_tags = filtered_question_tags.filter(lambda x: x is not None)                      \n",
    "q_tags = mapped_question_tags.filter(lambda x: x.PostTypeId==1)                      \n",
    "question_tags_df = q_tags.toDF(['Id','Tags', 'Body', 'PostTypeId'])                \n",
    " \n",
    "question_tags_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a876f14e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------+-------------------+\n",
      "|   Id|                Body|PostTypeId|                Tag|\n",
      "+-----+--------------------+----------+-------------------+\n",
      "|10893|I'm developing an...|         1|confidence-interval|\n",
      "|10897|I would like to c...|         1|        probability|\n",
      "|10900|I have to generat...|         1|      distributions|\n",
      "|10900|I have to generat...|         1|         randomness|\n",
      "|10905|I have searched f...|         1|                  r|\n",
      "|10905|I have searched f...|         1|        mixed-model|\n",
      "|10905|I have searched f...|         1|   cross-validation|\n",
      "|10905|I have searched f...|         1|      biostatistics|\n",
      "|10907|I have data with ...|         1|         regression|\n",
      "|10907|I have data with ...|         1|     basic-concepts|\n",
      "|10907|I have data with ...|         1|              error|\n",
      "|10907|I have data with ...|         1|      least-squares|\n",
      "|10910|I have a 37-quest...|         1|               spss|\n",
      "|10910|I have a 37-quest...|         1|             scales|\n",
      "|10918|I'm trying to do ...|         1|                  r|\n",
      "|10920|I am looking for ...|         1|           variance|\n",
      "|10926|As question, I ha...|         1|                  r|\n",
      "|10926|As question, I ha...|         1|confidence-interval|\n",
      "|10926|As question, I ha...|         1|         count-data|\n",
      "|10928|If i test two hyp...|         1|        probability|\n",
      "+-----+--------------------+----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, explode\n",
    "\n",
    "# flatten the list of tags to be able to count individual occurences of every tag\n",
    "flattened_tags = question_tags_df.withColumn(\"Tag\", explode(col(\"tags\"))).drop(\"tags\")\n",
    "\n",
    "flattened_tags.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8edcf5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42057"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpoint - number of total training posts\n",
    "\n",
    "flattened_tags.select('body').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1fa4a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116001"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_tags.select('body').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e288701",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|               Tag|count|\n",
      "+------------------+-----+\n",
      "|                 r| 7121|\n",
      "|        regression| 5408|\n",
      "|       time-series| 2654|\n",
      "|  machine-learning| 2524|\n",
      "|       probability| 2055|\n",
      "|hypothesis-testing| 1926|\n",
      "|     distributions| 1807|\n",
      "|        self-study| 1762|\n",
      "|          logistic| 1627|\n",
      "|       correlation| 1544|\n",
      "+------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find the top 10 tags based on their counts\n",
    "\n",
    "unique_word_count = flattened_tags.groupBy(\"Tag\").count()\n",
    "sorted_word_count = unique_word_count.orderBy('Count', ascending = False)\n",
    "sorted_word_count.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3dd5f8f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------+-------------------+---------+\n",
      "|   Id|                Body|PostTypeId|                Tag|In_top_10|\n",
      "+-----+--------------------+----------+-------------------+---------+\n",
      "|10893|I'm developing an...|         1|confidence-interval|       no|\n",
      "|10897|I would like to c...|         1|        probability|      yes|\n",
      "|10900|I have to generat...|         1|      distributions|      yes|\n",
      "|10900|I have to generat...|         1|         randomness|       no|\n",
      "|10905|I have searched f...|         1|                  r|      yes|\n",
      "|10905|I have searched f...|         1|        mixed-model|       no|\n",
      "|10905|I have searched f...|         1|   cross-validation|       no|\n",
      "|10905|I have searched f...|         1|      biostatistics|       no|\n",
      "|10907|I have data with ...|         1|         regression|      yes|\n",
      "|10907|I have data with ...|         1|     basic-concepts|       no|\n",
      "|10907|I have data with ...|         1|              error|       no|\n",
      "|10907|I have data with ...|         1|      least-squares|       no|\n",
      "|10910|I have a 37-quest...|         1|               spss|       no|\n",
      "|10910|I have a 37-quest...|         1|             scales|       no|\n",
      "|10918|I'm trying to do ...|         1|                  r|      yes|\n",
      "|10920|I am looking for ...|         1|           variance|       no|\n",
      "|10926|As question, I ha...|         1|                  r|      yes|\n",
      "|10926|As question, I ha...|         1|confidence-interval|       no|\n",
      "|10926|As question, I ha...|         1|         count-data|       no|\n",
      "|10928|If i test two hyp...|         1|        probability|      yes|\n",
      "+-----+--------------------+----------+-------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# list of top10 tags to check for in 'tag' column  of 'flattened_tags' dataframe\n",
    "top10 = ['r', 'regression', 'time-series', 'machine-learning', 'probability', 'hypothesis-testing',\\\n",
    "         'distributions', 'self-study', 'logistic', 'correlation']\n",
    "\n",
    "# add new 'top10' column based on top10 presence\n",
    "df_with_top10 = flattened_tags.withColumn(\"In_top_10\", when(col(\"Tag\").isin(top10),'yes').otherwise('no'))\n",
    "\n",
    "\n",
    "df_with_top10.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6772b6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|   Id|                Body|      In_top_10_list|\n",
      "+-----+--------------------+--------------------+\n",
      "|   39|I'm looking for w...|    [no, no, no, no]|\n",
      "|  269|What is the diffe...|    [no, no, no, no]|\n",
      "| 1142|I am working with...|   [yes, no, no, no]|\n",
      "| 4239|I am designing a ...|        [no, no, no]|\n",
      "| 4354|I was wondering i...|               [yes]|\n",
      "| 5359|<blockquote>  Dia...|                [no]|\n",
      "| 5807|Colleagues of min...|      [yes, yes, no]|\n",
      "| 6111|Here is the probl...|       [yes, no, no]|\n",
      "| 6582|When deconstructi...|        [no, no, no]|\n",
      "| 8559|Related to <a hre...|        [no, no, no]|\n",
      "| 9715|I ran a multinomi...|[yes, yes, no, no...|\n",
      "|10234|What is the diffe...|           [yes, no]|\n",
      "|11609|My current unders...|                [no]|\n",
      "|13038|Could someone exp...|           [yes, no]|\n",
      "|13193|I have to simulat...|            [no, no]|\n",
      "|15648|I have a dataset ...|       [yes, no, no]|\n",
      "|16321|Can we say anythi...|           [yes, no]|\n",
      "|16480|I have a dataset ...|       [yes, no, no]|\n",
      "|16834|I'm using Friedma...|    [no, no, no, no]|\n",
      "|17029|I wonder if <a hr...|                [no]|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_df = df_with_top10.groupBy('Id','Body').agg(F.collect_list(\"In_top_10\").alias(\"In_top_10_list\"))\n",
    "\n",
    "grouped_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6109362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42065"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3121f5ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-----+\n",
      "|   Id|                Body|label|\n",
      "+-----+--------------------+-----+\n",
      "|   39|I'm looking for w...|    0|\n",
      "|  269|What is the diffe...|    0|\n",
      "| 1142|I am working with...|    1|\n",
      "| 4239|I am designing a ...|    0|\n",
      "| 4354|I was wondering i...|    1|\n",
      "| 5359|<blockquote>  Dia...|    0|\n",
      "| 5807|Colleagues of min...|    1|\n",
      "| 6111|Here is the probl...|    1|\n",
      "| 6582|When deconstructi...|    0|\n",
      "| 8559|Related to <a hre...|    0|\n",
      "| 9715|I ran a multinomi...|    1|\n",
      "|10234|What is the diffe...|    1|\n",
      "|11609|My current unders...|    0|\n",
      "|13038|Could someone exp...|    1|\n",
      "|13193|I have to simulat...|    0|\n",
      "|15648|I have a dataset ...|    1|\n",
      "|16321|Can we say anythi...|    1|\n",
      "|16480|I have a dataset ...|    1|\n",
      "|16834|I'm using Friedma...|    0|\n",
      "|17029|I wonder if <a hr...|    0|\n",
      "+-----+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = grouped_df.withColumn(\"label\", F.when(F.array_contains(\"In_top_10_list\", 'yes') == True, 1)\\\n",
    "                     .otherwise(0))\\\n",
    "                     .drop(\"In_top_10_list\")\n",
    "\n",
    "train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "27b02922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posts with top 10 tag: 22525.\n",
      "posts without top 10 tag: 19540.\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint - Number of training posts with a tag in the top 10: 22525\n",
    "# Checkpoint - Number without: 19540\n",
    "\n",
    "print(f\"posts with top 10 tag: {train_df.select('body').where(train_df.label==1).count()}.\")\n",
    "print(f\"posts without top 10 tag: {train_df.select('body').where(train_df.label==0).count()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5029f864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-----+---------+------+\n",
      "|   Id|                Body|label|int_label|int_Id|\n",
      "+-----+--------------------+-----+---------+------+\n",
      "|   39|I'm looking for w...|    0|        0|    39|\n",
      "|  269|What is the diffe...|    0|        0|   269|\n",
      "| 1142|I am working with...|    1|        1|  1142|\n",
      "| 4239|I am designing a ...|    0|        0|  4239|\n",
      "| 4354|I was wondering i...|    1|        1|  4354|\n",
      "| 5359|<blockquote>  Dia...|    0|        0|  5359|\n",
      "| 5807|Colleagues of min...|    1|        1|  5807|\n",
      "| 6111|Here is the probl...|    1|        1|  6111|\n",
      "| 6582|When deconstructi...|    0|        0|  6582|\n",
      "| 8559|Related to <a hre...|    0|        0|  8559|\n",
      "| 9715|I ran a multinomi...|    1|        1|  9715|\n",
      "|10234|What is the diffe...|    1|        1| 10234|\n",
      "|11609|My current unders...|    0|        0| 11609|\n",
      "|13038|Could someone exp...|    1|        1| 13038|\n",
      "|13193|I have to simulat...|    0|        0| 13193|\n",
      "|15648|I have a dataset ...|    1|        1| 15648|\n",
      "|16321|Can we say anythi...|    1|        1| 16321|\n",
      "|16480|I have a dataset ...|    1|        1| 16480|\n",
      "|16834|I'm using Friedma...|    0|        0| 16834|\n",
      "|17029|I wonder if <a hr...|    0|        0| 17029|\n",
      "+-----+--------------------+-----+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train = train_df\n",
    "train = train.withColumn('int_label', col('label').cast('int')) \n",
    "train = train.withColumn('int_Id', col('Id').cast('int'))\n",
    "train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fb58bcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0efc8454",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol='Body', outputCol='words')\n",
    "count_vecrorizer = CountVectorizer(inputCol=tokenizer.getOutputCol(), outputCol='features')\n",
    "logreg = LogisticRegression(maxIter=24, regParam=1.0).setLabelCol('int_label')\n",
    "\n",
    "tokens = tokenizer.transform(train)\n",
    "count_model = count_vecrorizer.fit(tokens)\n",
    "counts = count_model.transform(tokens)\n",
    "model = logreg.fit(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c546ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import namedtuple\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "import re\n",
    "\n",
    "# allows to convert RDD to spark dataframe\n",
    "sqlContext = SQLContext(sc)    \n",
    "\n",
    "# read the train data\n",
    "test_data = sc.textFile('spark-stats-data/test/')\n",
    "\n",
    "# pull out only tags from every line of posts \n",
    "question_tag_tuple = namedtuple('question_tag_tuple', ['Id', 'Body', 'PostTypeId'])         \n",
    "\n",
    "def parse_question_tag(post):   \n",
    "    try:\n",
    "        tree = ET.fromstring(post.encode('utf-8'))   \n",
    "        post_id = int(tree.attrib.get('Id', 0))\n",
    "        body = tree.attrib.get('Body')\n",
    "        body = re.sub(r'<p>|</p>|<i>|</i>|\\n', '', body)\n",
    "        post_type = int(tree.attrib.get('PostTypeId'))\n",
    "        return question_tag_tuple(post_id, body, post_type)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "445600c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tags = test_data.filter(lambda line: line.strip().startswith('<row'))\\\n",
    "                     .map(parse_question_tag)\\\n",
    "                     .filter(lambda x: x is not None)\\\n",
    "                     .filter(lambda x: x.PostTypeId==1)\\\n",
    "                     .toDF(['Id', 'Body', 'PostTypeId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "20331274",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|   Id|                Body|\n",
      "+-----+--------------------+\n",
      "|10904|What are the pros...|\n",
      "|11000|I'd like to regre...|\n",
      "|11018|Let's aim for som...|\n",
      "|11019|I am doing text c...|\n",
      "|11079|I need an help be...|\n",
      "|11087|I was just wonder...|\n",
      "|11093|Apologies for wha...|\n",
      "|11109|If you have a var...|\n",
      "|11118|Given a data fram...|\n",
      "|11200|Today I opened tw...|\n",
      "|11219|I have been readi...|\n",
      "|11232|I am trying to co...|\n",
      "|11290|I have come acros...|\n",
      "|11296|I have a table  w...|\n",
      "|11315|So when I assume ...|\n",
      "|11381|I've calculated t...|\n",
      "|11435|Suppose $x_{1}, x...|\n",
      "|11516|I am doing Cox re...|\n",
      "|11531|I am looking at t...|\n",
      "|11568|I have one data s...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop 'PostTypeId' column to have only 'body' and 'id' \n",
    "test_df = test_tags.drop('PostTypeId')\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4d5d3e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on test documents\n",
    "test_tokens = tokenizer.transform(test_df)\n",
    "test_counts = count_model.transform(test_tokens)\n",
    "\n",
    "predictions = model.transform(test_counts)\n",
    "predictions = predictions.sort('Id', ascending=True)\n",
    "selected = predictions.select('Id','Body', 'prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "538748cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4649\n",
      "+---+--------------------+----------+\n",
      "| Id|                Body|prediction|\n",
      "+---+--------------------+----------+\n",
      "| 11|Is there a good, ...|       0.0|\n",
      "| 40|What algorithms a...|       0.0|\n",
      "| 47|I have a dataset ...|       0.0|\n",
      "| 93|We're trying to u...|       0.0|\n",
      "|183|I need to analyze...|       1.0|\n",
      "|212|I have 2 ASR (Aut...|       0.0|\n",
      "|216|What are some goo...|       1.0|\n",
      "|223|I have a friend w...|       0.0|\n",
      "|278|When a non-hierar...|       0.0|\n",
      "|290|I know of Cameron...|       0.0|\n",
      "|312|I'm a physics gra...|       0.0|\n",
      "|328|I realize that th...|       1.0|\n",
      "|354|Why do we seek to...|       1.0|\n",
      "|362|What is the diffe...|       0.0|\n",
      "|363|If you could go b...|       1.0|\n",
      "|373|From Wikipedia :<...|       1.0|\n",
      "|492|I am proposing to...|       0.0|\n",
      "|498|Sometimes, I just...|       1.0|\n",
      "|539|In answering <a h...|       0.0|\n",
      "|624|In engineering, w...|       0.0|\n",
      "+---+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(selected.count())\n",
    "selected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b110a6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: long (nullable = true)\n",
      " |-- Body: string (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the datatype of every column\n",
    "selected.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "00f147c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataframe to RDD, and choose the third column only ('prediction') \n",
    "result = selected.rdd.map(lambda x: (x[2])).collect()\n",
    "\n",
    "# make sure the datatype of values in list is in int\n",
    "classification = [int(x) for x in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "743f8070",
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 0.8659\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "#classification = [0] * 4649\n",
    "\n",
    "grader.score('spark_ml__classification', classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137e73ec",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## K-means (ungraded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b75688c",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "From your trained Word2Vec model, pass the vectors into a K-means clustering algorithm. Create a plot of the sum of squared error by calculating the square root of the sum of the squared distances for each point and its assigned cluster. For an independent variable use either the number of clusters k or the dimension of the Word2Vec vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef6fd99",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "*Copyright &copy; 2023 Pragmatic Institute. This content is licensed solely for personal use. Redistribution or publication of this material is strictly prohibited.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
